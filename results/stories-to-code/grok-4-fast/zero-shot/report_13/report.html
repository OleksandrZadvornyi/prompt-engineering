
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Analysis Report: Run 13</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f9f9f9;
            color: #333;
        }
        .container {
            max-width: 900px;
            margin: 20px auto;
            padding: 24px;
            background-color: #ffffff;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        h1, h2, h3 {
            color: #111;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h1 {
            font-size: 2em;
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
        }
        h2 {
            font-size: 1.5em;
            color: #555;
        }
        h3 {
            font-size: 1.2em;
            border-bottom: 1px solid #eee;
            padding-bottom: 5px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }
        th {
            background-color: #f7f7f7;
            font-weight: 600;
            width: 30%;
        }
        td {
            background-color: #fff;
        }
        details {
            border: 1px solid #ddd;
            border-radius: 4px;
            margin-bottom: 10px;
            background-color: #fff;
        }
        summary {
            padding: 12px;
            font-weight: 600;
            cursor: pointer;
            background-color: #f7f7f7;
        }
        summary:hover {
            background-color: #eee;
        }
        pre {
            background-color: #fdfdfd;
            border-top: 1px solid #eee;
            padding: 15px;
            margin: 0;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            font-size: 0.9em;
            line-height: 1.6;
        }
        code {
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
        }
        .credibility-score {
            font-size: 1.8em;
            font-weight: bold;
            color: #005fdd;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Analysis Report #13</h1>
        <h2>x-ai/grok-4-fast</h2>
        <p><strong>Prompt Variant:</strong> zero-shot</p>

        <h3>Total Credibility Score</h3>
        <p class="credibility-score">32.84%</p>

        
    <h3>Key Metrics</h3>
    <table>
        <tr><th>Perplexity</th><td>1.1407</td></tr>
<tr><th>Avg. Probability</th><td>0.8767</td></tr>
<tr><th>Total Tokens</th><td>6070</td></tr>

    </table>
    
        
    <h3>Execution Metrics</h3>
    <table>
        <tr><th>Execution Success</th><td>False</td></tr>
<tr><th>Execution Time Sec</th><td>2.6180</td></tr>
<tr><th>Exception Type</th><td>FileNotFoundError</td></tr>
<tr><th>Exception Message</th><td>[Errno 2] No such file or directory: 'historical_fabs.csv'</td></tr>
<tr><th>Runtime Output</th><td>2025-10-25 18:21:58,623 - __main__ - INFO - Deleted 3 records
Traceback (most recent call last):
  File "/app/main.py", line 518, in <module>
    app.historical.load_historical_fabs('historical_fabs.csv')
  File "/app/main.py", line 397, in load_historical_fabs
    df = pd.read_csv(file_path)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 1880, in _make_engine
    self.handles = get_handle(
                   ^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/pandas/io/common.py", line 873, in get_handle
    handle = open(
             ^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'historical_fabs.csv'</td></tr>

    </table>
    
        
    <h3>Structural Metrics</h3>
    <table>
        <tr><th>Avg Cyclomatic Complexity</th><td>2.2653</td></tr>
<tr><th>Ast Depth</th><td>13</td></tr>
<tr><th>Avg Function Size Lines</th><td>10.0698</td></tr>
<tr><th>Import Redundancy Ratio</th><td>0.0000</td></tr>

    </table>
    
        
    <h3>Semantic Metrics</h3>
    <table>
        <tr><th>Syntax Valid</th><td>True</td></tr>
<tr><th>Flake8 Error Count</th><td>63</td></tr>
<tr><th>Mypy Error Count</th><td>2</td></tr>

    </table>
    

        <details>
            <summary>View Full Prompt</summary>
            <pre>Generate fully functional Python code that implements the following user stories. The code should realistically reflect the described functionality.

ï»¿As a Data user, I want to have the 12-19-2017 deletions processed.
As a UI designer, I want to redesign the Resources page, so that it matches the new Broker design styles.
As a UI designer, I want to report to the Agencies about user testing, so that they are aware of their contributions to making Broker a better UX.
As a UI designer, I want to move on to round 2 of DABS or FABS landing page edits, so that I can get approvals from leadership.
As a UI designer, I want to move on to round 2 of Homepage edits, so that I can get approvals from leadership.
As a UI designer, I want to move on to round 3 of the Help page edits, so that I can get approvals from leadership.
As a Developer , I want to be able to log better, so that I can troubleshoot issues with particular submissions and functions.
As a Developer, I want to add the updates on a FABS submission to be modified when the publishStatus changes, so that I know when the status of the submission has changed.
As a DevOps engineer, I want New Relic to provide useful data across all applications.
As a UI designer,  I want to move on to round 2 of the Help page edits, so that I can get approvals from leadership.
As a UI designer, I want to move on to round 2 of Homepage edits, so that I can get approvals from leadership.
As a Broker user, I want to Upload and Validate the error message to have accurate text.
As a Broker user, I want the D1 file generation to be synced with the FPDS data load, so that I don&#x27;t have to regenerate a file if no data has been updated.
As a Website user, I want to access published FABS files, so that I can see the new files as they come in.
As an owner, I want to be sure that USAspending only send grant records to my system.
As a Developer, I want to update the Broker validation rule table to account for the rule updates in DB-2213.
As a Developer, I want to add the GTAS window data to the database, so that I can ensure the site is locked down during the GTAS submission period.
As a Developer, I want D Files generation requests to be managed and cached, so that duplicate requests do not cause performance issues.
As a user, I want to access the raw agency published files from FABS via USAspending.
As an Agency user, I want to be able to include a large number of flexfields without performance impact.
As a Broker user, I want  to help create content mockups, so that I can submit my data efficiently.
As a UI designer, I want to track the issues that come up in Tech Thursday, so that I know what to test and what want s to be fixed.
As an Owner, I want to create a user testing summary from the UI SME, so that I can know what UI improvements we will follow through on.
As a UI designer, I want to begin user testing, so that I can validate stakeholder UI improvement requests.
As a UI designer, I want to schedule user testing, so that I can give the testers advanced notice to ensure buy-in.
As an Owner, I want to design a schedule from the UI SME, so that I know the potential timeline of the UI improvements wanted.
As an Owner, I want to design an audit from the UI SME, so that I know the potential scope of the UI improvements want ed.
As a Developer, I want to prevent users from double publishing FABS submissions after refreshing, so that there are no duplicates.
As an data user, I want to receive updates to FABS records.
As an Agency user, I want to be able to include a large number of flexfields without performance impact.
As a Developer , I want to update the FABS sample file to remove FundingAgencyCode after FABS is updated to no longer require the header.
As an agency user, I want to ensure that deleted FSRS records are not included in submissions.
As a website user, I want to see updated financial assistance data daily.
As a user, I want the publish button in FABS to deactivate after I click it while the derivations are happening, so that I cannot click it multiple times for the same submission.
As a Developer , I want to ensure that attempts to correct or delete non-existent records don&#x27;t create new published data.
As an Owner, I want to reset the environment to only take Staging MAX permissions, so that I can ensure that the FABS testers no longer have access.
As a user, I want the flexfields in my submission file to appear in the warning and error files when the only error is a missing required element.
As a user, I want to have accurate and complete data related to PPoPCode and PPoPCongressionalDistrict.
As an agency user, I want the FABS validation rules to accept zero and blank for loan records.
As an Agency user, I want FABS deployed into production, so I can submit my Financial Assistance data.
As a Developer , I want to clarify to users what exactly is triggering the CFDA error code in each case.
As an agency user, I want to be confident that the data coming from SAM is complete.
As a Developer , I want my domain models to be indexed properly, so that I can get validation results back in a reasonable amount of time.
As an agency user, I want the FABS validation rules to accept zero and blank for non-loan records.
As a broker team member, I want to make some updates to the SQL codes for clarity.
As an agency user, I want to have all derived data elements derived properly.
As a broker team member, I want to add the 00***** and 00FORGN PPoPCode cases to the derivation logic.
As a data user, I want to see the office names derived from office codes, so that I can have appropriate context for understanding them.
As a broker user, I want the historical FABS loader to derive fields, so that my agency codes are correct in the PublishedAwardFinancialAssistance table.
As a broker team member, I want to ensure the Broker resources, validations, and P&amp;P pages are updated appropriately for the launch of FABS and DAIMS v1.1.
As a Developer, I want the data loaded from historical FABS to include the FREC derivations, so that I can have consistent FREC data for USASpending.gov.
As a user, I don&#x27;t want to see NASA grants displayed as contracts.
As a user, I want the DUNS validations to accept records whose ActionTypes are B, C, or D and the DUNS is registered in SAM, even though it may have expired.
As a user, I want the DUNS validations to accept records whose ActionDates are before the current registration date in SAM, but after the initial registration date.
As a broker team member, I want to derive FundingAgencyCode, so that the data quality and completeness improves.
As an agency user, I want the maximum length allowed for LegalEntityAddressLine3 to match Schema v1.1.
As an agency user, I want to use the schema v1.1 headers in my FABS file.
As a agency user, I want to map the FederalActionObligation properly to the Atom Feed.
As a Broker user, I want to have PPoPZIP+4 work the same as the Legal Entity ZIP validations.
As a FABS user, I want to link the SAMPLE FILE on the &quot;What you want  to submit&quot; dialog to point to the correct file, so that I have an accurate reference for my agency submissions.
As an Agency user, I want FPDS data to be up-to-date daily.
As a user, I want to access the raw agency published files from FABS via USAspending.
As a Developer , I want to determine how agencies will generate and validate D Files from FABS and FPDS data.
As a user, I want to generate and validate D Files from FABS and FPDS data.
As an Agency user, I want the header information box to show updated date AND time, so that I know when it was updated.
As an Agency user, I want to receive a more helpful file-level error when I upload a file with the wrong extension.
As a tester, I want to have access to test features in environments other than Staging, so that I can test any nonProd feature in any environment.
As a FABS user, I want to submission errors to accurately represent FABS errors, so that I know why my submission didn&#x27;t work.
As a FABS user, I want the frontend URLs to more accurately reflect the page I&#x27;m accessing, so that I&#x27;m not confused.
As an Agency user, I want all historical Financial Assistance data loaded for FABS go-live.
As a Developer , I want the historical FPDS data loader to include both extracted historical data and FPDS feed data.
As an Agency user, I want historical FPDS data loaded.
As an Agency user, I want to accurately see who created a submission, so that I&#x27;m not confused about who last updated a submission.
As an agency user, I want to get File F in the correct format.
As an Agency user, I want to better understand my file-level errors.
As a Developer , I want to provide FABS groups that function under the FREC paradigm.
As a tester, I want to ensure that FABS is deriving fields properly through a robust test file plus a follow up check.
As an owner, I only want zero-padded fields, so that I can justify padding.
As a Broker user, I want to submit records for individual recipients without receiving a DUNS error.
As a user, I want more information about how many rows will be published prior to deciding whether to publish.
As a Developer, I want to prevent duplicate transactions from being published and deal with the time gap between validation and the publishing decision.
As a FABS user, I want to submit a citywide as a PPoPZIP and pass validations.
As a Broker user, I want to have updated error codes that accurately reflect the logic and provide enough information, so that I can fix my submission.
As an agency user, I want to leave off the last 4 digits of the ZIP without an error, so that I can complete my submissions.
As a FABS user, I want to make sure the historical data includes all necessary columns, so that the information in the database is correct.
As a data user, I want to access two additional fields from the FPDS data pull.
As a FABS user, I want additional helpful info in the submission dashboard, so that I can better manage submissions and IG requests.
As a FABS user, I want to download the uploaded FABS file, so that I can get the uploaded file.
As a Developer I want to quickly access Broker application data, so that I can investigate issues.
As a Developer , I want to determine the best way to load historical FPDS data, so that I can load all FPDS data since 2007.
As a FABS user, I want the language on FABS pages to be appropriate for me, so that I am not confused.
As a FABS user, I do not want  DABS banner messages and vice versa, so that I have the appropriate information for my application.
As a FABS user, I want to have read-only access to DABS, so that I can view DABS pages without wanting two sets of permissions.
As a FABS user, I want to have my validations run in a reasonable amount of time.
As a FABS user, I want to see correct status labels on the Submission Dashboard, so that I can quickly see my submission history.
As an agency user, I want to know when the submission periods start and end, so that I know when the submission starts and ends.
As an agency user, I want a landing page to navigate to either FABS or DABS pages, so that I can access both sides of the site.
As an agency user, I want to submit my data elements surrounded by quotation marks, so that Excel won&#x27;t strip off leading and trailing zeroes.

Output only Python code (no markdown formatting or extra text). Do not leave functions empty â implement reasonable logic where needed.</pre>
        </details>
        
        <details>
            <summary>View Generated Code</summary>
            <pre><code>import logging
import json
import sqlite3
from datetime import datetime, date
from functools import lru_cache
from pathlib import Path
import pandas as pd
from typing import Dict, List, Any, Optional
import hashlib
import re

# Configure logging for better troubleshooting
logging.basicConfig(level=logging.INFO, format=&#x27;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#x27;)
logger = logging.getLogger(__name__)

class DatabaseManager:
    &quot;&quot;&quot;Manages SQLite database for Broker and FABS data.&quot;&quot;&quot;
    
    def __init__(self, db_path: str = &#x27;broker.db&#x27;):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.init_db()
    
    def init_db(self):
        &quot;&quot;&quot;Initialize database schema for FABS, submissions, validations, etc.&quot;&quot;&quot;
        cursor = self.conn.cursor()
        # Table for submissions
        cursor.execute(&#x27;&#x27;&#x27;
            CREATE TABLE IF NOT EXISTS submissions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                agency_name TEXT,
                file_hash TEXT UNIQUE,
                status TEXT DEFAULT &#x27;pending&#x27;,
                publish_status TEXT DEFAULT &#x27;draft&#x27;,
                created_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        &#x27;&#x27;&#x27;)
        # Table for validation rules
        cursor.execute(&#x27;&#x27;&#x27;
            CREATE TABLE IF NOT EXISTS validation_rules (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                rule_code TEXT UNIQUE,
                description TEXT,
                is_active BOOLEAN DEFAULT TRUE
            )
        &#x27;&#x27;&#x27;)
        # Table for GTAS window data
        cursor.execute(&#x27;&#x27;&#x27;
            CREATE TABLE IF NOT EXISTS gtas_windows (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                start_date DATE,
                end_date DATE,
                is_locked BOOLEAN DEFAULT FALSE
            )
        &#x27;&#x27;&#x27;)
        # Table for published awards
        cursor.execute(&#x27;&#x27;&#x27;
            CREATE TABLE IF NOT EXISTS published_awards (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                submission_id INTEGER,
                frec_code TEXT,
                action_obligation DECIMAL(15,2),
                ppop_code TEXT,
                ppop_congressional_district TEXT,
                duns TEXT,
                action_type TEXT,
                action_date DATE,
                FOREIGN KEY (submission_id) REFERENCES submissions (id)
            )
        &#x27;&#x27;&#x27;)
        # Insert sample validation rules (updated for DB-2213)
        cursor.execute(&quot;INSERT OR IGNORE INTO validation_rules (rule_code, description) VALUES (&#x27;CFDA_MISSING&#x27;, &#x27;CFDA code is required for non-loan records&#x27;)&quot;)
        cursor.execute(&quot;INSERT OR IGNORE INTO validation_rules (rule_code, description) VALUES (&#x27;DUNS_INVALID&#x27;, &#x27;DUNS must be registered in SAM&#x27;)&quot;)
        cursor.execute(&quot;INSERT OR IGNORE INTO validation_rules (rule_code, description) VALUES (&#x27;ZIP_INVALID&#x27;, &#x27;ZIP+4 format must be valid&#x27;)&quot;)
        # Insert sample GTAS window
        cursor.execute(&quot;INSERT OR IGNORE INTO gtas_windows (start_date, end_date) VALUES (&#x27;2023-10-01&#x27;, &#x27;2023-10-31&#x27;)&quot;)
        self.conn.commit()
    
    def close(self):
        self.conn.close()
    
    def add_submission(self, agency: str, file_content: str, user: str) -&gt; int:
        &quot;&quot;&quot;Add a new submission, compute hash to prevent duplicates.&quot;&quot;&quot;
        file_hash = hashlib.md5(file_content.encode()).hexdigest()
        cursor = self.conn.cursor()
        cursor.execute(&#x27;&#x27;&#x27;
            INSERT OR IGNORE INTO submissions (agency_name, file_hash, created_by)
            VALUES (?, ?, ?)
        &#x27;&#x27;&#x27;, (agency, file_hash, user))
        self.conn.commit()
        return cursor.lastrowid
    
    def update_publish_status(self, submission_id: int, status: str):
        &quot;&quot;&quot;Update publish status and log the change.&quot;&quot;&quot;
        cursor = self.conn.cursor()
        cursor.execute(&#x27;&#x27;&#x27;
            UPDATE submissions SET publish_status = ?, updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        &#x27;&#x27;&#x27;, (status, submission_id))
        self.conn.commit()
        logger.info(f&quot;Publish status updated for submission {submission_id} to {status}&quot;)
    
    def get_submission(self, sub_id: int) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Retrieve submission details.&quot;&quot;&quot;
        cursor = self.conn.cursor()
        cursor.execute(&#x27;SELECT * FROM submissions WHERE id = ?&#x27;, (sub_id,))
        row = cursor.fetchone()
        if row:
            return {
                &#x27;id&#x27;: row[0], &#x27;agency&#x27;: row[1], &#x27;file_hash&#x27;: row[2], &#x27;status&#x27;: row[3],
                &#x27;publish_status&#x27;: row[4], &#x27;created_by&#x27;: row[5], &#x27;created_at&#x27;: row[6], &#x27;updated_at&#x27;: row[7]
            }
        return {}
    
    def insert_published_award(self, submission_id: int, data: Dict[str, Any]):
        &quot;&quot;&quot;Insert derived published award data, including FREC derivations.&quot;&quot;&quot;
        cursor = self.conn.cursor()
        # Derive FREC (simplified logic)
        frec = self._derive_frec(data.get(&#x27;funding_agency_code&#x27;, &#x27;&#x27;), data.get(&#x27;office_code&#x27;, &#x27;&#x27;))
        # Derive PPoPCode and Congressional District
        ppop_code = data.get(&#x27;ppop_zip&#x27;, &#x27;&#x27;).replace(&#x27; &#x27;, &#x27;&#x27;)[:5] if data.get(&#x27;ppop_zip&#x27;) else &#x27;00000&#x27;
        ppop_cong_district = self._derive_congressional_district(ppop_code)
        # Ensure zero-padded fields
        action_obligation = f&quot;{float(data.get(&#x27;action_obligation&#x27;, 0)):.2f}&quot;
        cursor.execute(&#x27;&#x27;&#x27;
            INSERT INTO published_awards (submission_id, frec_code, action_obligation, ppop_code, ppop_congressional_district, duns, action_type, action_date)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        &#x27;&#x27;&#x27;, (submission_id, frec, action_obligation, ppop_code, ppop_cong_district, data.get(&#x27;duns&#x27;), data.get(&#x27;action_type&#x27;), data.get(&#x27;action_date&#x27;)))
        self.conn.commit()
    
    def _derive_frec(self, agency_code: str, office_code: str) -&gt; str:
        &quot;&quot;&quot;Derive FREC from agency and office codes (simplified).&quot;&quot;&quot;
        if agency_code.startswith(&#x27;00&#x27;) or office_code.startswith(&#x27;00FORGN&#x27;):
            return f&quot;{agency_code[:2]}{office_code[:3]}&quot;
        return f&quot;{agency_code[:4]}&quot;
    
    def _derive_congressional_district(self, zip_code: str) -&gt; str:
        &quot;&quot;&quot;Derive congressional district from ZIP (mock logic).&quot;&quot;&quot;
        # Mock: Use ZIP mod 10 for district
        return str(int(zip_code[-1]) % 10)
    
    def is_gtas_locked(self) -&gt; bool:
        &quot;&quot;&quot;Check if current date is within locked GTAS window.&quot;&quot;&quot;
        today = date.today()
        cursor = self.conn.cursor()
        cursor.execute(&#x27;SELECT is_locked FROM gtas_windows WHERE start_date &lt;= ? AND end_date &gt;= ?&#x27;, (today, today))
        row = cursor.fetchone()
        return row[0] if row else False
    
    def delete_nonexistent_records(self, record_ids: List[int]):
        &quot;&quot;&quot;Process deletions like 12-19-2017, ensure no new data created.&quot;&quot;&quot;
        cursor = self.conn.cursor()
        for rid in record_ids:
            cursor.execute(&#x27;DELETE FROM published_awards WHERE id = ?&#x27;, (rid,))
        self.conn.commit()
        logger.info(f&quot;Deleted {len(record_ids)} records&quot;)

class ValidationEngine:
    &quot;&quot;&quot;Handles FABS validation rules, updated for DB-2213.&quot;&quot;&quot;
    
    def __init__(self, db_manager: DatabaseManager):
        self.db = db_manager
        self.rules = self._load_rules()
    
    def _load_rules(self) -&gt; Dict[str, Dict[str, str]]:
        cursor = self.db.conn.cursor()
        cursor.execute(&#x27;SELECT rule_code, description FROM validation_rules WHERE is_active = TRUE&#x27;)
        return {row[0]: {&#x27;desc&#x27;: row[1]} for row in cursor.fetchall()}
    
    def validate_file(self, file_content: str, is_loan: bool = False) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Validate FABS file content, accept zero/blank for loans/non-loans.&quot;&quot;&quot;
        errors = []
        warnings = []
        df = pd.read_csv(pd.StringIO(file_content))
        
        for idx, row in df.iterrows():
            # CFDA validation (clarify triggers)
            if pd.isna(row.get(&#x27;cfda_code&#x27;)) and not is_loan:
                errors.append({&#x27;row&#x27;: idx, &#x27;code&#x27;: &#x27;CFDA_MISSING&#x27;, &#x27;msg&#x27;: &#x27;CFDA required for non-loan records&#x27;})
            elif is_loan and (row.get(&#x27;cfda_code&#x27;) == 0 or pd.isna(row.get(&#x27;cfda_code&#x27;))):
                pass  # Accept for loans
            # DUNS validation (accept expired if registered, for certain actions)
            duns = str(row.get(&#x27;duns&#x27;, &#x27;&#x27;)).strip()
            action_type = row.get(&#x27;action_type&#x27;, &#x27;&#x27;)
            if action_type in [&#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;] and self._is_duns_registered(duns):
                pass  # Accept even if expired
            elif not self._is_duns_registered(duns):
                errors.append({&#x27;row&#x27;: idx, &#x27;code&#x27;: &#x27;DUNS_INVALID&#x27;, &#x27;msg&#x27;: f&#x27;DUNS {duns} not registered or expired improperly&#x27;})
            # ZIP validation (PPoPZIP+4 like Legal Entity, accept incomplete ZIP)
            zip_code = str(row.get(&#x27;ppop_zip&#x27;, &#x27;&#x27;)).strip()
            if len(zip_code) &gt;= 5 and not re.match(r&#x27;^\d{5}(-\d{4})?$&#x27;, zip_code):
                errors.append({&#x27;row&#x27;: idx, &#x27;code&#x27;: &#x27;ZIP_INVALID&#x27;, &#x27;msg&#x27;: &#x27;Invalid ZIP+4 format&#x27;})
            elif len(zip_code) &lt; 5:
                warnings.append({&#x27;row&#x27;: idx, &#x27;code&#x27;: &#x27;ZIP_INCOMPLETE&#x27;, &#x27;msg&#x27;: &#x27;ZIP too short, using default&#x27;})
            # LegalEntityAddressLine3 max length (Schema v1.1: assume 55 chars)
            addr3 = str(row.get(&#x27;legal_entity_address_line3&#x27;, &#x27;&#x27;))
            if len(addr3) &gt; 55:
                errors.append({&#x27;row&#x27;: idx, &#x27;code&#x27;: &#x27;ADDR3_TOO_LONG&#x27;, &#x27;msg&#x27;: &#x27;Address line 3 exceeds 55 chars&#x27;})
            # Flexfields (handle large number without impact, just log)
            flexfields = row.get(&#x27;flexfields&#x27;, [])
            if len(flexfields) &gt; 100:
                warnings.append({&#x27;row&#x27;: idx, &#x27;code&#x27;: &#x27;FLEXFIELDS_LARGE&#x27;, &#x27;msg&#x27;: f&#x27;{len(flexfields)} flexfields detected&#x27;})
            # ActionObligation mapping to Atom Feed (simplified check)
            obligation = row.get(&#x27;federal_action_obligation&#x27;)
            if obligation and not isinstance(obligation, (int, float)):
                errors.append({&#x27;row&#x27;: idx, &#x27;code&#x27;: &#x27;OBLIGATION_INVALID&#x27;, &#x27;msg&#x27;: &#x27;Obligation must be numeric&#x27;})
        
        # Flexfields in errors/warnings if only missing required
        if errors and all(e[&#x27;code&#x27;] == &#x27;REQUIRED_MISSING&#x27; for e in errors):
            # Add flexfields info (mock)
            for e in errors:
                e[&#x27;flexfields&#x27;] = flexfields[:5]  # First 5 for example
        
        # Update rule table if needed (for DB-2213)
        self._update_rule_table()
        
        return {
            &#x27;errors&#x27;: errors,
            &#x27;warnings&#x27;: warnings,
            &#x27;valid_rows&#x27;: len(df) - len(errors),
            &#x27;total_rows&#x27;: len(df),
            &#x27;error_file&#x27;: self._generate_error_file(errors, df),
            &#x27;warning_file&#x27;: self._generate_error_file(warnings, df)
        }
    
    def _is_duns_registered(self, duns: str) -&gt; bool:
        &quot;&quot;&quot;Mock SAM check: assume registered if 9 digits.&quot;&quot;&quot;
        return bool(re.match(r&#x27;^\d{9}$&#x27;, duns))
    
    def _update_rule_table(self):
        &quot;&quot;&quot;Update validation rules for DB-2213 (e.g., add loan zero accept).&quot;&quot;&quot;
        cursor = self.db.conn.cursor()
        cursor.execute(&#x27;&#x27;&#x27;
            INSERT OR IGNORE INTO validation_rules (rule_code, description)
            VALUES (&#x27;LOAN_ZERO_ACCEPT&#x27;, &#x27;Accept zero/blank for loan records&#x27;)
        &#x27;&#x27;&#x27;)
        self.db.conn.commit()
    
    def _generate_error_file(self, issues: List[Dict], df: pd.DataFrame) -&gt; str:
        &quot;&quot;&quot;Generate error/warning file with flexfields if applicable.&quot;&quot;&quot;
        if not issues:
            return &#x27;&#x27;
        error_df = pd.DataFrame(issues)
        error_df[&#x27;flexfields_sample&#x27;] = &#x27;N/A&#x27;  # Placeholder
        return error_df.to_csv(index=False)
    
    @lru_cache(maxsize=128)
    def validate_duns_historical(self, duns: str, action_date: str) -&gt; bool:
        &quot;&quot;&quot;Cached DUNS validation for historical data (accept if before current reg but after initial).&quot;&quot;&quot;
        reg_date = &#x27;2010-01-01&#x27;  # Mock initial reg
        expiry_date = datetime.now().strftime(&#x27;%Y-%m-%d&#x27;)
        parsed_date = datetime.strptime(action_date, &#x27;%Y-%m-%d&#x27;).date()
        current_reg = datetime.now().date()
        return parsed_date &gt; datetime.strptime(reg_date, &#x27;%Y-%m-%d&#x27;).date() and parsed_date &lt; current_reg

class FABSProcessor:
    &quot;&quot;&quot;Main processor for FABS submissions, derivations, publishing.&quot;&quot;&quot;
    
    def __init__(self, db_manager: DatabaseManager):
        self.db = db_manager
        self.validator = ValidationEngine(db_manager)
        self.is_publishing = False  # To prevent double publishing
    
    def process_submission(self, agency: str, file_path: str, user: str, is_loan: bool = False) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Process FABS file: validate, derive, publish if valid.&quot;&quot;&quot;
        with open(file_path, &#x27;r&#x27;) as f:
            content = f.read()
        
        sub_id = self.db.add_submission(agency, content, user)
        validation = self.validator.validate_file(content, is_loan)
        
        if not validation[&#x27;errors&#x27;]:
            # Derive fields for each row
            df = pd.read_csv(pd.StringIO(content))
            for _, row in df.iterrows():
                derived_data = self._derive_fields(row.to_dict())
                # Ensure no double publish
                if self.is_publishing:
                    logger.warning(&quot;Publish attempt during ongoing publish - skipping&quot;)
                    continue
                self.is_publishing = True
                try:
                    self.db.insert_published_award(sub_id, derived_data)
                    self.db.update_publish_status(sub_id, &#x27;published&#x27;)
                finally:
                    self.is_publishing = False
            # Update FABS records (for data users)
            self._update_fabs_records(sub_id)
        else:
            self.db.update_publish_status(sub_id, &#x27;failed&#x27;)
            # Accurate error messages
            logger.error(f&quot;Submission {sub_id} failed: {len(validation[&#x27;errors&#x27;])} errors&quot;)
        
        # Handle flexfields in large numbers (no impact, just process)
        # Download uploaded file (return path or content)
        return {
            &#x27;submission_id&#x27;: sub_id,
            &#x27;validation&#x27;: validation,
            &#x27;uploaded_file&#x27;: file_path,
            &#x27;dashboard_info&#x27;: self._get_dashboard_info(sub_id)
        }
    
    def _derive_fields(self, row: Dict[str, Any]) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Derive fields: FundingAgencyCode, PPoPCode, office names, etc.&quot;&quot;&quot;
        # Derive FundingAgencyCode (improves quality)
        funding_agency = row.get(&#x27;funding_agency_code&#x27;, &#x27;0000&#x27;)
        row[&#x27;derived_funding_agency&#x27;] = funding_agency
        # Office name from code (mock)
        office_code = row.get(&#x27;office_code&#x27;, &#x27;&#x27;)
        row[&#x27;office_name&#x27;] = f&quot;Office_{office_code}&quot; if office_code else &#x27;Unknown&#x27;
        # PPoPCode for 00***** and 00FORGN
        ppop_zip = row.get(&#x27;ppop_zip&#x27;, &#x27;&#x27;).upper()
        if ppop_zip.startswith(&#x27;00&#x27;) or &#x27;FORGN&#x27; in ppop_zip:
            row[&#x27;ppop_code&#x27;] = &#x27;00FORGN&#x27;
        # FREC derivations for historical loader
        row[&#x27;frec&#x27;] = self._derive_frec_internal(row)
        # Ensure all derived properly for agency user
        row[&#x27;legal_entity_zip&#x27;] = self._format_zip(row.get(&#x27;legal_entity_zip&#x27;, &#x27;&#x27;))
        # Map FederalActionObligation (ensure numeric)
        row[&#x27;federal_action_obligation&#x27;] = float(row.get(&#x27;federal_action_obligation&#x27;, 0))
        return row
    
    def _derive_frec_internal(self, row: Dict) -&gt; str:
        &quot;&quot;&quot;Internal FREC derivation for consistency.&quot;&quot;&quot;
        agency = row.get(&#x27;agency_code&#x27;, &#x27;&#x27;)
        return agency[:4] if len(agency) &gt;= 4 else &#x27;0000&#x27;
    
    def _format_zip(self, zip_str: str) -&gt; str:
        &quot;&quot;&quot;Format ZIP+4, accept citywide or incomplete.&quot;&quot;&quot;
        zip_str = re.sub(r&#x27;\s+&#x27;, &#x27;&#x27;, str(zip_str))
        if len(zip_str) == 5:
            return zip_str + &#x27;-0000&#x27;  # Pad if incomplete
        return zip_str[:9] if len(zip_str) &gt; 9 else zip_str
    
    def _update_fabs_records(self, sub_id: int):
        &quot;&quot;&quot;Update FABS records for data users, receive updates.&quot;&quot;&quot;
        # Mock: Just log for now
        logger.info(f&quot;Updated FABS records for submission {sub_id}&quot;)
    
    def _get_dashboard_info(self, sub_id: int) -&gt; str:
        &quot;&quot;&quot;Additional helpful info for submission dashboard.&quot;&quot;&quot;
        sub = self.db.get_submission(sub_id)
        return json.dumps({
            &#x27;status&#x27;: sub[&#x27;status&#x27;],
            &#x27;publish_status&#x27;: sub[&#x27;publish_status&#x27;],
            &#x27;created_by&#x27;: sub[&#x27;created_by&#x27;],
            &#x27;rows_to_publish&#x27;: 100,  # Mock count before publish decision
            &#x27;ig_requests&#x27;: 0
        })
    
    def prevent_duplicate_publish(self, sub_id: int) -&gt; bool:
        &quot;&quot;&quot;Prevent double publishing after refresh.&quot;&quot;&quot;
        sub = self.db.get_submission(sub_id)
        if sub[&#x27;publish_status&#x27;] == &#x27;published&#x27;:
            logger.warning(f&quot;Duplicate publish attempt for {sub_id}&quot;)
            return False
        return True
    
    def generate_d_file(self, fabs_data: Dict, fpds_data: Dict) -&gt; str:
        &quot;&quot;&quot;Generate and cache D file from FABS and FPDS, synced with load.&quot;&quot;&quot;
        # Cache key
        cache_key = hashlib.md5(json.dumps({**fabs_data, **fpds_data}).encode()).hexdigest()
        if hasattr(self, &#x27;_d_cache&#x27;) and cache_key in self._d_cache:
            logger.info(&quot;Returning cached D file&quot;)
            return self._d_cache[cache_key]
        
        # Mock generation: Combine and validate
        combined = {**fabs_data, **fpds_data}
        d_content = json.dumps(combined)  # Simplified
        self._d_cache = getattr(self, &#x27;_d_cache&#x27;, {})
        self._d_cache[cache_key] = d_content
        # Ensure synced: Check if FPDS updated
        if not self._is_fpds_updated():
            logger.info(&quot;No FPDS update, using sync&quot;)
        return d_content
    
    def _is_fpds_updated(self) -&gt; bool:
        &quot;&quot;&quot;Mock FPDS load sync.&quot;&quot;&quot;
        return False  # Assume no update
    
    @property
    def _d_cache(self):
        if not hasattr(self, &#x27;__d_cache&#x27;):
            self.__d_cache = {}
        return self.__d_cache

class HistoricalDataLoader:
    &quot;&quot;&quot;Loads historical FABS and FPDS data, includes derivations.&quot;&quot;&quot;
    
    def __init__(self, db_manager: DatabaseManager):
        self.db = db_manager
    
    def load_historical_fabs(self, file_path: str):
        &quot;&quot;&quot;Load historical FABS, derive fields, ensure all columns, no NASA grants as contracts.&quot;&quot;&quot;
        df = pd.read_csv(file_path)
        # Remove FundingAgencyCode header if updated
        if &#x27;FundingAgencyCode&#x27; in df.columns:
            df = df.drop(&#x27;FundingAgencyCode&#x27;, axis=1)
        # Derive fields
        processor = FABSProcessor(self.db)
        for _, row in df.iterrows():
            derived = processor._derive_fields(row.to_dict())
            # Ensure FREC
            derived[&#x27;frec&#x27;] = processor._derive_frec_internal(derived)
            # Insert only if not contract-like for grants
            if derived.get(&#x27;award_type&#x27;) == &#x27;grant&#x27; and not derived.get(&#x27;is_contract&#x27;, False):
                # Mock insert
                pass
        # Ensure deleted FSRS records not included
        self._filter_deleted_fsrs(df)
        logger.info(f&quot;Loaded historical FABS from {file_path}, rows: {len(df)}&quot;)
    
    def load_historical_fpds(self, start_year: int = 2007):
        &quot;&quot;&quot;Load historical FPDS since 2007, include extracted and feed data.&quot;&quot;&quot;
        # Mock: Load from files or API
        years = range(start_year, date.today().year + 1)
        for year in years:
            # Simulate loading
            fpds_data = {&#x27;year&#x27;: year, &#x27;records&#x27;: []}
            # Add two additional fields
            fpds_data[&#x27;additional_field1&#x27;] = &#x27;value1&#x27;
            fpds_data[&#x27;additional_field2&#x27;] = &#x27;value2&#x27;
            # Daily up-to-date
            self.db.conn.execute(&#x27;INSERT INTO published_awards (frec_code, action_date) VALUES (?, ?)&#x27;,
                                 (&#x27;FPDS&#x27;, f&#x27;{year}-01-01&#x27;))
        self.db.conn.commit()
        logger.info(f&quot;Loaded historical FPDS from {start_year}&quot;)
    
    def _filter_deleted_fsrs(self, df: pd.DataFrame):
        &quot;&quot;&quot;Ensure deleted FSRS records not included.&quot;&quot;&quot;
        df = df[df[&#x27;status&#x27;] != &#x27;deleted&#x27;]  # Mock filter

class BrokerApp:
    &quot;&quot;&quot;Main Broker application, handles logging, caching, etc.&quot;&quot;&quot;
    
    def __init__(self):
        self.db = DatabaseManager()
        self.fabs = FABSProcessor(self.db)
        self.historical = HistoricalDataLoader(self.db)
        self.d_cache = {}  # For D files management
    
    def upload_and_validate(self, file_path: str, agency: str, user: str, is_loan: bool = False):
        &quot;&quot;&quot;Upload, validate with accurate messages, handle wrong extension.&quot;&quot;&quot;
        path = Path(file_path)
        if not path.suffix.lower() in [&#x27;.csv&#x27;, &#x27;.txt&#x27;]:
            return {&#x27;error&#x27;: &#x27;Wrong file extension. Use .csv or .txt&#x27;}
        return self.fabs.process_submission(agency, file_path, user, is_loan)
    
    def publish_submission(self, sub_id: int):
        &quot;&quot;&quot;Publish with deactivation, prevent multiples, show row count.&quot;&quot;&quot;
        if not self.fabs.prevent_duplicate_publish(sub_id):
            return {&#x27;error&#x27;: &#x27;Already published or in progress&#x27;}
        sub = self.db.get_submission(sub_id)
        row_count = 100  # Mock
        if row_count == 0:
            return {&#x27;warning&#x27;: &#x27;No rows to publish&#x27;}
        # Proceed with publish logic (already in process_submission)
        self.db.update_publish_status(sub_id, &#x27;publishing&#x27;)
        logger.info(f&quot;Publishing {row_count} rows for {sub_id}&quot;)
        return {&#x27;success&#x27;: True, &#x27;rows_published&#x27;: row_count}
    
    def generate_sample_file(self) -&gt; str:
        &quot;&quot;&quot;Generate FABS sample file without FundingAgencyCode header.&quot;&quot;&quot;
        sample_data = [
            {&#x27;cfda_code&#x27;: &#x27;12.001&#x27;, &#x27;duns&#x27;: &#x27;123456789&#x27;, &#x27;action_type&#x27;: &#x27;A&#x27;, &#x27;federal_action_obligation&#x27;: 1000.00,
             &#x27;ppop_zip&#x27;: &#x27;12345-6789&#x27;, &#x27;legal_entity_address_line3&#x27;: &#x27;Short addr&#x27;}
        ]
        df = pd.DataFrame(sample_data)
        # No FundingAgencyCode
        return df.to_csv(index=False)
    
    def access_raw_fabs_files(self, agency: str) -&gt; List[str]:
        &quot;&quot;&quot;Access published FABS files via USAspending, daily updates.&quot;&quot;&quot;
        # Mock: Return file paths
        return [f&#x27;/files/{agency}_fabs_{datetime.now().date()}.csv&#x27;]
    
    def ensure_grant_only(self):
        &quot;&quot;&quot;Ensure only grant records sent (owner requirement).&quot;&quot;&quot;
        cursor = self.db.conn.cursor()
        cursor.execute(&quot;DELETE FROM published_awards WHERE award_type != &#x27;grant&#x27;&quot;)
        self.db.conn.commit()
        logger.info(&quot;Filtered to grant records only&quot;)
    
    def reset_environment(self, permissions: str = &#x27;Staging MAX&#x27;):
        &quot;&quot;&quot;Reset env to staging permissions only.&quot;&quot;&quot;
        # Mock: Log
        logger.info(f&quot;Environment reset to {permissions} permissions&quot;)
    
    def load_12_19_2017_deletions(self):
        &quot;&quot;&quot;Process 12-19-2017 deletions.&quot;&quot;&quot;
        record_ids = [1, 2, 3]  # Mock IDs
        self.db.delete_nonexistent_records(record_ids)
    
    def update_fabs_sample_remove_header(self):
        &quot;&quot;&quot;Update sample file to remove FundingAgencyCode.&quot;&quot;&quot;
        sample = self.generate_sample_file()
        logger.info(&quot;Updated FABS sample file&quot;)

# DevOps: Mock New Relic data
class NewRelicMonitor:
    &quot;&quot;&quot;Provide useful data across apps (mock).&quot;&quot;&quot;
    
    def get_metrics(self, app_name: str) -&gt; Dict[str, Any]:
        return {
            &#x27;app&#x27;: app_name,
            &#x27;response_time&#x27;: 200.5,
            &#x27;error_rate&#x27;: 0.01,
            &#x27;throughput&#x27;: 100
        }

# Entry point for testing
if __name__ == &quot;__main__&quot;:
    app = BrokerApp()
    # Example usage
    app.load_12_19_2017_deletions()
    app.historical.load_historical_fabs(&#x27;historical_fabs.csv&#x27;)
    app.historical.load_historical_fpds()
    result = app.upload_and_validate(&#x27;sample.csv&#x27;, &#x27;NASA&#x27;, &#x27;dev_user&#x27;)
    print(json.dumps(result, indent=2))
    app.publish_submission(result[&#x27;submission_id&#x27;])
    app.ensure_grant_only()
    app.reset_environment()
    monitor = NewRelicMonitor()
    print(monitor.get_metrics(&#x27;FABS&#x27;))
    app.db.close()</code></pre>
        </details>
    </div>
</body>
</html>
    

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Analysis Report: Run 5</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f9f9f9;
            color: #333;
        }
        .container {
            max-width: 900px;
            margin: 20px auto;
            padding: 24px;
            background-color: #ffffff;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        h1, h2, h3 {
            color: #111;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h1 {
            font-size: 2em;
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
        }
        h2 {
            font-size: 1.5em;
            color: #555;
        }
        h3 {
            font-size: 1.2em;
            border-bottom: 1px solid #eee;
            padding-bottom: 5px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }
        th {
            background-color: #f7f7f7;
            font-weight: 600;
            width: 30%;
        }
        td {
            background-color: #fff;
        }
        details {
            border: 1px solid #ddd;
            border-radius: 4px;
            margin-bottom: 10px;
            background-color: #fff;
        }
        summary {
            padding: 12px;
            font-weight: 600;
            cursor: pointer;
            background-color: #f7f7f7;
        }
        summary:hover {
            background-color: #eee;
        }
        pre {
            background-color: #fdfdfd;
            border-top: 1px solid #eee;
            padding: 15px;
            margin: 0;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            font-size: 0.9em;
            line-height: 1.6;
        }
        code {
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
        }
        .credibility-score {
            font-size: 1.8em;
            font-weight: bold;
            color: #005fdd;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Analysis Report #5</h1>
        <h2>x-ai/grok-4-fast</h2>
        <p><strong>Prompt Variant:</strong> zero-shot-clusters</p>

        <h3>Total Credibility Score</h3>
        <p class="credibility-score">30.90%</p>

        
    <h3>Key Metrics</h3>
    <table>
        <tr><th>Perplexity</th><td>1.1105</td></tr>
<tr><th>Avg. Probability</th><td>0.9005</td></tr>
<tr><th>Total Tokens</th><td>8337</td></tr>

    </table>
    
        
    <h3>Execution Metrics</h3>
    <table>
        <tr><th>Execution Success</th><td>False</td></tr>
<tr><th>Execution Time Sec</th><td>1.7420</td></tr>
<tr><th>Exception Type</th><td>sqlite3.OperationalError</td></tr>
<tr><th>Exception Message</th><td>no such column: record_data</td></tr>
<tr><th>Runtime Output</th><td>2025-10-25 18:23:17,026 - INFO - Resources page redesigned to match Broker styles
2025-10-25 18:23:17,027 - INFO - Processed 0 deletions for 2017-12-19
2025-10-25 18:23:17,027 - INFO - New Relic configured for all apps
2025-10-25 18:23:17,028 - INFO - Homepage, Help, DABS/FABS landing pages edited - ready for approval
2025-10-25 18:23:17,028 - INFO - Enhanced logging for submissions
Traceback (most recent call last):
  File "/app/main.py", line 804, in <module>
    app.run_all()
  File "/app/main.py", line 791, in run_all
    self.process_cluster_5()
  File "/app/main.py", line 598, in process_cluster_5
    self.submission_mgr.access_raw_agency_files('001')
  File "/app/main.py", line 471, in access_raw_agency_files
    cursor.execute("SELECT file_path FROM submissions WHERE agency_id = ? AND publish_status = 'published' AND record_data LIKE '%grant%'", (agency_id,))
sqlite3.OperationalError: no such column: record_data</td></tr>

    </table>
    
        
    <h3>Structural Metrics</h3>
    <table>
        <tr><th>Avg Cyclomatic Complexity</th><td>2.6207</td></tr>
<tr><th>Ast Depth</th><td>14</td></tr>
<tr><th>Avg Function Size Lines</th><td>14.2400</td></tr>
<tr><th>Import Redundancy Ratio</th><td>0.0000</td></tr>

    </table>
    
        
    <h3>Semantic Metrics</h3>
    <table>
        <tr><th>Syntax Valid</th><td>True</td></tr>
<tr><th>Flake8 Error Count</th><td>20</td></tr>
<tr><th>Mypy Error Count</th><td>8</td></tr>

    </table>
    

        <details>
            <summary>View Full Prompt</summary>
            <pre>Generate fully functional Python code that implements the following user stories. The code should realistically reflect the described functionality.

Cluster (4,):
As a Data user, I want to have the 12-19-2017 deletions processed. As a UI designer, I want to redesign the Resources page, so that it matches the new Broker design styles. As a UI designer, I want to report to the Agencies about user testing, so that they are aware of their contributions to making Broker a better UX. As a DevOps engineer, I want New Relic to provide useful data across all applications. As a Broker user, I want the D1 file generation to be synced with the FPDS data load, so that I don&#x27;t have to regenerate a file if no data has been updated. As a broker team member, I want to make some updates to the SQL codes for clarity. As a broker team member, I want to add the 00***** and 00FORGN PPoPCode cases to the derivation logic. As a broker team member, I want to derive FundingAgencyCode, so that the data quality and completeness improves. As a agency user, I want to map the FederalActionObligation properly to the Atom Feed. As a Broker user, I want to have PPoPZIP+4 work the same as the Legal Entity ZIP validations.

Cluster (5,):
As a UI designer, I want to move on to round 2 of DABS or FABS landing page edits, so that I can get approvals from leadership. As a UI designer, I want to move on to round 2 of Homepage edits, so that I can get approvals from leadership. As a UI designer, I want to move on to round 3 of the Help page edits, so that I can get approvals from leadership. As a Developer , I want to be able to log better, so that I can troubleshoot issues with particular submissions and functions. As a UI designer,  I want to move on to round 2 of the Help page edits, so that I can get approvals from leadership. As a UI designer, I want to move on to round 2 of Homepage edits, so that I can get approvals from leadership. As a Website user, I want to access published FABS files, so that I can see the new files as they come in. As an owner, I want to be sure that USAspending only send grant records to my system. As a Broker user, I want  to help create content mockups, so that I can submit my data efficiently. As a UI designer, I want to track the issues that come up in Tech Thursday, so that I know what to test and what want s to be fixed. As an Owner, I want to create a user testing summary from the UI SME, so that I can know what UI improvements we will follow through on. As a UI designer, I want to begin user testing, so that I can validate stakeholder UI improvement requests. As a UI designer, I want to schedule user testing, so that I can give the testers advanced notice to ensure buy-in. As an Owner, I want to design a schedule from the UI SME, so that I know the potential timeline of the UI improvements wanted. As an Owner, I want to design an audit from the UI SME, so that I know the potential scope of the UI improvements want ed. As an Owner, I want to reset the environment to only take Staging MAX permissions, so that I can ensure that the FABS testers no longer have access. As a Developer , I want my domain models to be indexed properly, so that I can get validation results back in a reasonable amount of time. As an Agency user, I want the header information box to show updated date AND time, so that I know when it was updated. As an owner, I only want zero-padded fields, so that I can justify padding. As a Broker user, I want to have updated error codes that accurately reflect the logic and provide enough information, so that I can fix my submission. As a Developer I want to quickly access Broker application data, so that I can investigate issues. As a FABS user, I want to have read-only access to DABS, so that I can view DABS pages without wanting two sets of permissions. As an agency user, I want a landing page to navigate to either FABS or DABS pages, so that I can access both sides of the site.

Cluster (2,):
As a Developer, I want to add the updates on a FABS submission to be modified when the publishStatus changes, so that I know when the status of the submission has changed. As a Developer, I want to add the GTAS window data to the database, so that I can ensure the site is locked down during the GTAS submission period. As a Developer , I want to update the FABS sample file to remove FundingAgencyCode after FABS is updated to no longer require the header. As a user, I want the publish button in FABS to deactivate after I click it while the derivations are happening, so that I cannot click it multiple times for the same submission. As a broker user, I want the historical FABS loader to derive fields, so that my agency codes are correct in the PublishedAwardFinancialAssistance table. As a Developer, I want the data loaded from historical FABS to include the FREC derivations, so that I can have consistent FREC data for USASpending.gov. As a FABS user, I want the frontend URLs to more accurately reflect the page I&#x27;m accessing, so that I&#x27;m not confused. As a Developer , I want the historical FPDS data loader to include both extracted historical data and FPDS feed data. As a Developer , I want to provide FABS groups that function under the FREC paradigm. As a FABS user, I want to make sure the historical data includes all necessary columns, so that the information in the database is correct. As a data user, I want to access two additional fields from the FPDS data pull. As a FABS user, I want additional helpful info in the submission dashboard, so that I can better manage submissions and IG requests. As a FABS user, I want to download the uploaded FABS file, so that I can get the uploaded file. As a Developer , I want to determine the best way to load historical FPDS data, so that I can load all FPDS data since 2007. As a FABS user, I want the language on FABS pages to be appropriate for me, so that I am not confused. As a FABS user, I do not want  DABS banner messages and vice versa, so that I have the appropriate information for my application. As an agency user, I want to know when the submission periods start and end, so that I know when the submission starts and ends.

Cluster (0,):
As a Broker user, I want to Upload and Validate the error message to have accurate text. As a Developer, I want to update the Broker validation rule table to account for the rule updates in DB-2213. As a user, I want the flexfields in my submission file to appear in the warning and error files when the only error is a missing required element. As a Developer , I want to clarify to users what exactly is triggering the CFDA error code in each case. As a broker team member, I want to ensure the Broker resources, validations, and P&amp;P pages are updated appropriately for the launch of FABS and DAIMS v1.1. As a user, I want the DUNS validations to accept records whose ActionTypes are B, C, or D and the DUNS is registered in SAM, even though it may have expired.  As a user, I want the DUNS validations to accept records whose ActionDates are before the current registration date in SAM, but after the initial registration date. As an Agency user, I want to receive a more helpful file-level error when I upload a file with the wrong extension. As a Developer, I want to prevent duplicate transactions from being published and deal with the time gap between validation and the publishing decision.

Cluster (1,):
As a Developer, I want D Files generation requests to be managed and cached, so that duplicate requests do not cause performance issues. As a user, I want to access the raw agency published files from FABS via USAspending. As an Agency user, I want to be able to include a large number of flexfields without performance impact. As a Developer, I want to prevent users from double publishing FABS submissions after refreshing, so that there are no duplicates. As an Agency user, I want to be able to include a large number of flexfields without performance impact. As a website user, I want to see updated financial assistance data daily. As a Developer , I want to ensure that attempts to correct or delete non-existent records don&#x27;t create new published data. As a user, I want to have accurate and complete data related to PPoPCode and PPoPCongressionalDistrict. As a user, I don&#x27;t want to see NASA grants displayed as contracts. As a user, I want to access the raw agency published files from FABS via USAspending. As a Developer , I want to determine how agencies will generate and validate D Files from FABS and FPDS data. As a user, I want to generate and validate D Files from FABS and FPDS data. As a tester, I want to have access to test features in environments other than Staging, so that I can test any nonProd feature in any environment. As a FABS user, I want to submission errors to accurately represent FABS errors, so that I know why my submission didn&#x27;t work. As an Agency user, I want to accurately see who created a submission, so that I&#x27;m not confused about who last updated a submission. As a tester, I want to ensure that FABS is deriving fields properly through a robust test file plus a follow up check. As a Broker user, I want to submit records for individual recipients without receiving a DUNS error. As a user, I want more information about how many rows will be published prior to deciding whether to publish. As a FABS user, I want to submit a citywide as a PPoPZIP and pass validations. As a FABS user, I want to have my validations run in a reasonable amount of time.

Cluster (3,):
As an data user, I want to receive updates to FABS records. As an agency user, I want to ensure that deleted FSRS records are not included in submissions. As an agency user, I want the FABS validation rules to accept zero and blank for loan records. As an Agency user, I want FABS deployed into production, so I can submit my Financial Assistance data. As an agency user, I want to be confident that the data coming from SAM is complete. As an agency user, I want the FABS validation rules to accept zero and blank for non-loan records. As an agency user, I want to have all derived data elements derived properly. As an agency user, I want the maximum length allowed for LegalEntityAddressLine3 to match Schema v1.1. As an agency user, I want to use the schema v1.1 headers in my FABS file. As an Agency user, I want FPDS data to be up-to-date daily. As an Agency user, I want all historical Financial Assistance data loaded for FABS go-live. As an Agency user, I want historical FPDS data loaded. As an agency user, I want to get File F in the correct format. As an Agency user, I want to better understand my file-level errors. As an agency user, I want to submit my data elements surrounded by quotation marks, so that Excel won&#x27;t strip off leading and trailing zeroes.

Cluster (2, 5):
As a data user, I want to see the office names derived from office codes, so that I can have appropriate context for understanding them.

Cluster (2, 4, 5):
As a FABS user, I want to link the SAMPLE FILE on the &quot;What you want  to submit&quot; dialog to point to the correct file, so that I have an accurate reference for my agency submissions.

Cluster (3, 5):
As an agency user, I want to leave off the last 4 digits of the ZIP without an error, so that I can complete my submissions.

Cluster (1, 2):
As a FABS user, I want to see correct status labels on the Submission Dashboard, so that I can quickly see my submission history.

Output only Python code (no markdown formatting or extra text). Do not leave functions empty — implement reasonable logic where needed.</pre>
        </details>
        
        <details>
            <summary>View Generated Code</summary>
            <pre><code>import sqlite3
import pandas as pd
from datetime import datetime, date
from typing import Dict, List, Optional, Tuple
import logging
import json
import hashlib
import os
from pathlib import Path

# Set up logging for better troubleshooting
logging.basicConfig(level=logging.INFO, format=&#x27;%(asctime)s - %(levelname)s - %(message)s&#x27;)
logger = logging.getLogger(__name__)

# Simulate database with SQLite for demo purposes
DB_PATH = &#x27;broker.db&#x27;

class BrokerDatabase:
    def __init__(self):
        self.conn = sqlite3.connect(DB_PATH)
        self.cursor = self.conn.cursor()
        self.init_db()

    def init_db(self):
        # Tables for submissions, historical data, validations, etc.
        self.cursor.execute(&#x27;&#x27;&#x27;
            CREATE TABLE IF NOT EXISTS submissions (
                id INTEGER PRIMARY KEY,
                agency_id TEXT,
                file_path TEXT,
                status TEXT DEFAULT &#x27;pending&#x27;,
                publish_status TEXT DEFAULT &#x27;unpublished&#x27;,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                creator_id TEXT,
                rows_published INTEGER DEFAULT 0
            )
        &#x27;&#x27;&#x27;)
        self.cursor.execute(&#x27;&#x27;&#x27;
            CREATE TABLE IF NOT EXISTS published_awards (
                id INTEGER PRIMARY KEY,
                submission_id INTEGER,
                record_data TEXT,  -- JSON for record
                derived_fields TEXT,
                FOREIGN KEY(submission_id) REFERENCES submissions(id)
            )
        &#x27;&#x27;&#x27;)
        self.cursor.execute(&#x27;&#x27;&#x27;
            CREATE TABLE IF NOT EXISTS historical_fabs (
                id INTEGER PRIMARY KEY,
                agency_code TEXT,
                frec_code TEXT,
                derived_agency TEXT,
                load_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        &#x27;&#x27;&#x27;)
        self.cursor.execute(&#x27;&#x27;&#x27;
            CREATE TABLE IF NOT EXISTS fpds_data (
                id INTEGER PRIMARY KEY,
                piid TEXT UNIQUE,
                action_obligation DECIMAL,
                funding_agency_code TEXT,
                pop_zip TEXT,
                load_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        &#x27;&#x27;&#x27;)
        self.cursor.execute(&#x27;&#x27;&#x27;
            CREATE TABLE IF NOT EXISTS sam_duns (
                duns TEXT PRIMARY KEY,
                registration_date DATE,
                expiration_date DATE,
                is_active BOOLEAN DEFAULT 1
            )
        &#x27;&#x27;&#x27;)
        self.cursor.execute(&#x27;&#x27;&#x27;
            CREATE TABLE IF NOT EXISTS gtas_windows (
                id INTEGER PRIMARY KEY,
                start_date DATE,
                end_date DATE,
                is_locked BOOLEAN DEFAULT 0
            )
        &#x27;&#x27;&#x27;)
        self.conn.commit()

    def close(self):
        self.conn.close()

# Global DB instance
db = BrokerDatabase()

class DerivationLogic:
    @staticmethod
    def derive_funding_agency_code(record: Dict) -&gt; str:
        &quot;&quot;&quot;Derive FundingAgencyCode for data quality improvement.&quot;&quot;&quot;
        # Simplified logic: based on agency code and type
        agency_code = record.get(&#x27;AgencyCode&#x27;, &#x27;&#x27;)
        if agency_code.startswith(&#x27;00&#x27;) or &#x27;FORGN&#x27; in agency_code:
            return f&quot;Derived_{agency_code[:2]}&quot;
        return agency_code or &#x27;Unknown&#x27;

    @staticmethod
    def derive_pop_code(ppop_zip: str, action_type: str) -&gt; str:
        &quot;&quot;&quot;Handle 00***** and 00FORGN PPoPCode cases, sync with ZIP+4 validations.&quot;&quot;&quot;
        if not ppop_zip:
            return &#x27;Unknown&#x27;
        # ZIP+4 validation similar to Legal Entity ZIP
        if len(ppop_zip) == 5:  # Allow without last 4 digits
            ppop_zip += &#x27;0000&#x27;
        if ppop_zip.startswith(&#x27;00&#x27;) or &#x27;FORGN&#x27; in ppop_zip:
            return f&quot;Derived_POP_{ppop_zip[:8]}&quot;
        # For citywide or individual recipients
        if action_type in [&#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;] or &#x27;citywide&#x27; in ppop_zip.lower():
            return &#x27;Valid_POP&#x27;
        return ppop_zip[:5] if len(ppop_zip) &gt; 5 else ppop_zip

    @staticmethod
    def derive_frec_code(agency_code: str, office_code: str) -&gt; str:
        &quot;&quot;&quot;Derive FREC from agency and office codes.&quot;&quot;&quot;
        # Simplified: concatenate and hash for uniqueness
        combined = f&quot;{agency_code}_{office_code}&quot;
        return hashlib.md5(combined.encode()).hexdigest()[:8].upper()

    @staticmethod
    def derive_office_name(office_code: str) -&gt; str:
        &quot;&quot;&quot;Derive office names from codes for context.&quot;&quot;&quot;
        # Mock mapping
        office_map = {
            &#x27;001&#x27;: &#x27;Office of Administration&#x27;,
            &#x27;002&#x27;: &#x27;Office of Finance&#x27;,
            # Add more as needed
        }
        return office_map.get(office_code, f&quot;Office_{office_code}&quot;)

    @staticmethod
    def map_federal_action_obligation(record: Dict) -&gt; Dict:
        &quot;&quot;&quot;Map FederalActionObligation to Atom Feed properly.&quot;&quot;&quot;
        obligation = record.get(&#x27;FederalActionObligation&#x27;, 0)
        if obligation == 0 and record.get(&#x27;RecordType&#x27;, &#x27;&#x27;) == &#x27;Loan&#x27;:
            obligation = None  # Accept zero/blank for loans
        return {&#x27;obligation&#x27;: obligation, &#x27;atom_feed_mapped&#x27;: True}

class ValidationEngine:
    def __init__(self):
        self.rules = self.load_validation_rules()

    def load_validation_rules(self) -&gt; Dict:
        &quot;&quot;&quot;Load updated validation rules for DB-2213 and v1.1.&quot;&quot;&quot;
        return {
            &#x27;duns&#x27;: {
                &#x27;required&#x27;: True,
                &#x27;sam_check&#x27;: True,
                &#x27;accept_expired&#x27;: [&#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;],
                &#x27;date_check&#x27;: True
            },
            &#x27;cfda&#x27;: {
                &#x27;triggers&#x27;: [&#x27;InvalidTitle&#x27;, &#x27;MissingProgram&#x27;]
            },
            &#x27;zip&#x27;: {
                &#x27;allow_5_digits&#x27;: True,
                &#x27;ppop_sync&#x27;: True
            },
            &#x27;flexfields&#x27;: {
                &#x27;max_count&#x27;: 100,  # Handle large without impact
                &#x27;include_in_errors&#x27;: True
            },
            &#x27;file_extension&#x27;: [&#x27;.csv&#x27;, &#x27;.xlsx&#x27;],
            &#x27;zero_padding&#x27;: True  # Only zero-padded fields
        }

    def validate_duns(self, duns: str, action_type: str, action_date: date) -&gt; Tuple[bool, str]:
        &quot;&quot;&quot;DUNS validations: accept registered even expired for B,C,D; date checks.&quot;&quot;&quot;
        if not duns:
            return False, &quot;Missing DUNS&quot;
        cursor = db.cursor
        cursor.execute(&quot;SELECT registration_date, expiration_date FROM sam_duns WHERE duns=?&quot;, (duns,))
        result = cursor.fetchone()
        if not result:
            return False, &quot;DUNS not in SAM&quot;
        reg_date, exp_date = result
        today = date.today()
        if action_type in [&#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;] and (exp_date and today &gt; exp_date):
            # Accept expired if registered
            return True, &quot;DUNS accepted (expired but registered)&quot;
        if action_date &lt; reg_date or (exp_date and action_date &gt; exp_date):
            return False, &quot;Action date outside SAM registration&quot;
        return True, &quot;Valid DUNS&quot;

    def validate_file_extension(self, file_path: str) -&gt; Tuple[bool, str]:
        &quot;&quot;&quot;Helpful file-level error for wrong extension.&quot;&quot;&quot;
        ext = Path(file_path).suffix.lower()
        if ext not in self.rules[&#x27;file_extension&#x27;]:
            return False, f&quot;Invalid file extension &#x27;{ext}&#x27;. Expected: {&#x27;, &#x27;.join(self.rules[&#x27;file_extension&#x27;])}&quot;
        return True, &quot;Valid extension&quot;

    def validate_record(self, record: Dict, flexfields: List[Dict] = None) -&gt; List[Dict]:
        &quot;&quot;&quot;Full record validation with flexfields, CFDA details, etc.&quot;&quot;&quot;
        errors = []
        # DUNS
        is_valid, msg = self.validate_duns(record.get(&#x27;DUNS&#x27;), record.get(&#x27;ActionType&#x27;), record.get(&#x27;ActionDate&#x27;))
        if not is_valid:
            errors.append({&#x27;code&#x27;: &#x27;DUNS_INVALID&#x27;, &#x27;message&#x27;: msg, &#x27;field&#x27;: &#x27;DUNS&#x27;})
        # ZIP for PPoP
        zip_code = record.get(&#x27;PPoPZIP&#x27;)
        if zip_code and len(zip_code) &lt; 9 and self.rules[&#x27;zip&#x27;][&#x27;allow_5_digits&#x27;]:
            record[&#x27;PPoPZIP&#x27;] = zip_code + &#x27;0000&#x27;  # Pad
        if len(flexfields or []) &gt; self.rules[&#x27;flexfields&#x27;][&#x27;max_count&#x27;]:
            errors.append({&#x27;code&#x27;: &#x27;FLEXFIELDS_EXCESS&#x27;, &#x27;message&#x27;: &#x27;Too many flexfields&#x27;})
        # CFDA error clarification
        cfda = record.get(&#x27;CFDA&#x27;)
        if not cfda:
            errors.append({&#x27;code&#x27;: &#x27;CFDA_MISSING&#x27;, &#x27;triggers&#x27;: self.rules[&#x27;cfda&#x27;][&#x27;triggers&#x27;]})
        elif &#x27;Invalid&#x27; in cfda:
            errors.append({&#x27;code&#x27;: &#x27;CFDA_INVALID&#x27;, &#x27;triggers&#x27;: [&#x27;TitleMismatch&#x27;, &#x27;ProgramNotFound&#x27;]})
        # Zero/blank for loans/non-loans
        if record.get(&#x27;RecordType&#x27;) == &#x27;Loan&#x27; and record.get(&#x27;SomeField&#x27;) in [0, &#x27;&#x27;]:
            pass  # Accept
        else:
            if record.get(&#x27;SomeField&#x27;) not in [0, &#x27;&#x27;, None]:
                errors.append({&#x27;code&#x27;: &#x27;NON_LOAN_ZERO_REQUIRED&#x27;})
        # Include flexfields in errors if only missing required
        if flexfields and errors and len(errors) == 1 and &#x27;missing required&#x27; in errors[0].get(&#x27;message&#x27;, &#x27;&#x27;).lower():
            for ff in flexfields[:5]:  # Sample
                errors.append({&#x27;code&#x27;: &#x27;FLEXFIELD_INCLUDED&#x27;, &#x27;data&#x27;: ff})
        # Zero padding justification
        for field in [&#x27;PIID&#x27;, &#x27;SomeCode&#x27;]:
            val = record.get(field)
            if val and self.rules[&#x27;zero_padding&#x27;] and not str(val).zfill(9).startswith(&#x27;0&#x27;):
                record[field] = str(val).zfill(9)
        return errors

    def validate_submission(self, file_path: str, is_fabs: bool = True) -&gt; Dict:
        &quot;&quot;&quot;Validate entire submission file, generate error/warning files.&quot;&quot;&quot;
        if not self.validate_file_extension(file_path)[0]:
            return {&#x27;errors&#x27;: [self.validate_file_extension(file_path)[1]], &#x27;rows&#x27;: 0}
        df = pd.read_csv(file_path) if file_path.endswith(&#x27;.csv&#x27;) else pd.read_excel(file_path)
        errors = []
        valid_rows = 0
        for idx, row in df.iterrows():
            record = row.to_dict()
            # Handle quoted fields for Excel zeroes
            for col in record:
                if isinstance(record[col], str) and record[col].startswith(&#x27;&quot;&#x27;) and record[col].endswith(&#x27;&quot;&#x27;):
                    record[col] = record[col][1:-1]
            flexfields = record.pop(&#x27;flexfields&#x27;, []) if isinstance(record.get(&#x27;flexfields&#x27;), list) else []
            errs = self.validate_record(record, flexfields)
            if errs:
                errors.extend([{&#x27;row&#x27;: idx+1, **err} for err in errs])
            else:
                valid_rows += 1
        # Generate error file
        if errors:
            error_df = pd.DataFrame(errors)
            error_file = file_path.replace(&#x27;.&#x27;, &#x27;_errors.&#x27;)
            error_df.to_csv(error_file, index=False)
            logger.info(f&quot;Error file generated: {error_file}&quot;)
        return {&#x27;errors&#x27;: errors, &#x27;rows_published&#x27;: valid_rows, &#x27;total_rows&#x27;: len(df)}

class SubmissionManager:
    def __init__(self):
        self.validation_engine = ValidationEngine()
        self.cache = {}  # For D Files caching
        self.is_gtas_locked = self.check_gtas_lock()

    def check_gtas_lock(self) -&gt; bool:
        &quot;&quot;&quot;Check if site is locked during GTAS submission.&quot;&quot;&quot;
        cursor = db.cursor
        cursor.execute(&quot;SELECT is_locked FROM gtas_windows WHERE start_date &lt;= ? AND end_date &gt;= ?&quot;, 
                       (date.today().isoformat(), date.today().isoformat()))
        result = cursor.fetchone()
        return bool(result[0]) if result else False

    def upload_and_validate(self, file_path: str, agency_id: str, creator_id: str, is_fabs: bool = True) -&gt; Dict:
        &quot;&quot;&quot;Upload, validate, handle flexfields, prevent duplicates.&quot;&quot;&quot;
        if self.is_gtas_locked:
            return {&#x27;error&#x27;: &#x27;Site locked during GTAS window&#x27;}
        # Check for duplicate upload hash
        file_hash = hashlib.md5(open(file_path, &#x27;rb&#x27;).read()).hexdigest()
        if file_hash in self.cache:
            return {&#x27;warning&#x27;: &#x27;Duplicate upload detected&#x27;, &#x27;cached&#x27;: True}
        self.cache[file_hash] = datetime.now()
        # Insert submission
        cursor = db.cursor
        cursor.execute(&quot;INSERT INTO submissions (agency_id, file_path, creator_id) VALUES (?, ?, ?)&quot;,
                       (agency_id, file_path, creator_id))
        submission_id = cursor.lastrowid
        db.conn.commit()
        # Validate
        validation_result = self.validation_engine.validate_submission(file_path, is_fabs)
        # Update rows to publish
        cursor.execute(&quot;UPDATE submissions SET rows_published = ? WHERE id = ?&quot;,
                       (validation_result[&#x27;rows_published&#x27;], submission_id))
        db.conn.commit()
        # More info on rows prior to publish
        logger.info(f&quot;Validation complete for submission {submission_id}: {validation_result[&#x27;rows_published&#x27;]} publishable rows&quot;)
        return {&#x27;submission_id&#x27;: submission_id, **validation_result, &#x27;updated_by&#x27;: creator_id}

    def publish_submission(self, submission_id: int, prevent_double: bool = True) -&gt; Dict:
        &quot;&quot;&quot;Publish with derivations, prevent double-click duplicates.&quot;&quot;&quot;
        cursor = db.cursor
        cursor.execute(&quot;SELECT status, publish_status, file_path FROM submissions WHERE id = ?&quot;, (submission_id,))
        sub = cursor.fetchone()
        if not sub:
            return {&#x27;error&#x27;: &#x27;Submission not found&#x27;}
        status, pub_status, file_path = sub
        if pub_status == &#x27;published&#x27;:
            return {&#x27;warning&#x27;: &#x27;Already published&#x27;}
        if prevent_double and status == &#x27;publishing&#x27;:
            return {&#x27;error&#x27;: &#x27;Already in progress - double publish prevented&#x27;}
        # Set status
        cursor.execute(&quot;UPDATE submissions SET status = &#x27;publishing&#x27;, publish_status = &#x27;publishing&#x27; WHERE id = ?&quot;, (submission_id,))
        db.conn.commit()
        # Deactivate publish button simulation - here just log
        logger.info(&quot;Publish button deactivated during derivations&quot;)
        df = pd.read_csv(file_path)
        for idx, row in df.iterrows():
            record = row.to_dict()
            # Derivations
            record[&#x27;FundingAgencyCode&#x27;] = DerivationLogic.derive_funding_agency_code(record)
            record[&#x27;PPoPCode&#x27;] = DerivationLogic.derive_pop_code(record.get(&#x27;PPoPZIP&#x27;), record.get(&#x27;ActionType&#x27;))
            record[&#x27;FREC&#x27;] = DerivationLogic.derive_frec_code(record.get(&#x27;AgencyCode&#x27;), record.get(&#x27;OfficeCode&#x27;))
            record[&#x27;OfficeName&#x27;] = DerivationLogic.derive_office_name(record.get(&#x27;OfficeCode&#x27;))
            DerivationLogic.map_federal_action_obligation(record)
            # Historical loader derivations
            if &#x27;historical&#x27; in file_path.lower():
                record[&#x27;AgencyCodesCorrect&#x27;] = True
            # Insert published
            cursor.execute(&quot;INSERT INTO published_awards (submission_id, record_data, derived_fields) VALUES (?, ?, ?)&quot;,
                           (submission_id, json.dumps(record), json.dumps({
                               &#x27;funding_agency&#x27;: record[&#x27;FundingAgencyCode&#x27;],
                               &#x27;pop_code&#x27;: record[&#x27;PPoPCode&#x27;],
                               &#x27;frec&#x27;: record[&#x27;FREC&#x27;]
                           })))
        # Update status
        cursor.execute(&quot;UPDATE submissions SET status = &#x27;published&#x27;, publish_status = &#x27;published&#x27;, updated_at = CURRENT_TIMESTAMP WHERE id = ?&quot;, (submission_id,))
        db.conn.commit()
        # Reset status for UI
        logger.info(&quot;Publish complete - status labels updated&quot;)
        return {&#x27;success&#x27;: True, &#x27;rows_published&#x27;: len(df)}

    def handle_deletions(self, date_str: str = &#x27;2017-12-19&#x27;) -&gt; int:
        &quot;&quot;&quot;Process deletions for specific date.&quot;&quot;&quot;
        deletion_date = datetime.strptime(date_str, &#x27;%Y-%m-%d&#x27;).date()
        cursor = db.cursor
        cursor.execute(&quot;DELETE FROM published_awards WHERE submission_id IN (SELECT id FROM submissions WHERE date(updated_at) = ?)&quot;, (deletion_date.isoformat(),))
        deleted_count = cursor.rowcount
        db.conn.commit()
        # Also prevent publishing deleted records
        logger.info(f&quot;Processed {deleted_count} deletions for {date_str}&quot;)
        return deleted_count

    def load_historical_fabs(self, data_source: str) -&gt; int:
        &quot;&quot;&quot;Load historical FABS with derivations, ensure all columns.&quot;&quot;&quot;
        # Simulate loading from file or feed
        sample_data = [
            {&#x27;agency_code&#x27;: &#x27;001&#x27;, &#x27;frec_code&#x27;: &#x27;&#x27;, &#x27;other_col&#x27;: &#x27;value&#x27;},
            {&#x27;agency_code&#x27;: &#x27;002&#x27;, &#x27;frec_code&#x27;: &#x27;&#x27;, &#x27;other_col&#x27;: &#x27;value2&#x27;}
        ]
        loaded = 0
        for rec in sample_data:
            frec = DerivationLogic.derive_frec_code(rec[&#x27;agency_code&#x27;], &#x27;001&#x27;)
            rec[&#x27;derived_agency&#x27;] = DerivationLogic.derive_funding_agency_code(rec)
            cursor = db.cursor
            cursor.execute(&quot;INSERT INTO historical_fabs (agency_code, frec_code, derived_agency) VALUES (?, ?, ?)&quot;,
                           (rec[&#x27;agency_code&#x27;], frec, rec[&#x27;derived_agency&#x27;]))
            loaded += 1
        db.conn.commit()
        return loaded

    def load_historical_fpds(self, since_year: int = 2007) -&gt; int:
        &quot;&quot;&quot;Load historical FPDS from extracted and feed data, add two fields.&quot;&quot;&quot;
        # Simulate
        sample_fpds = [
            {&#x27;piid&#x27;: &#x27;A123&#x27;, &#x27;action_obligation&#x27;: 1000.0, &#x27;funding_agency_code&#x27;: &#x27;001&#x27;, &#x27;pop_zip&#x27;: &#x27;12345&#x27;, &#x27;extra_field1&#x27;: &#x27;val1&#x27;, &#x27;extra_field2&#x27;: &#x27;val2&#x27;}
        ]
        loaded = 0
        for rec in sample_fpds:
            rec[&#x27;pop_code&#x27;] = DerivationLogic.derive_pop_code(rec[&#x27;pop_zip&#x27;], &#x27;A&#x27;)
            cursor = db.cursor
            cursor.execute(&quot;INSERT OR REPLACE INTO fpds_data (piid, action_obligation, funding_agency_code, pop_zip, load_date) VALUES (?, ?, ?, ?, ?)&quot;,
                           (rec[&#x27;piid&#x27;], rec[&#x27;action_obligation&#x27;], rec[&#x27;funding_agency_code&#x27;], rec[&#x27;pop_code&#x27;], datetime.now()))
            loaded += 1
        db.conn.commit()
        # Sync D1 with FPDS load
        self.generate_d1_file()
        return loaded

    def generate_d1_file(self, cache_requests: bool = True) -&gt; str:
        &quot;&quot;&quot;Generate D1 file synced with FPDS load, managed/cached.&quot;&quot;&quot;
        cache_key = &#x27;d1_latest&#x27;
        if cache_requests and cache_key in self.cache:
            return self.cache[cache_key]
        # Query latest FPDS and FABS
        cursor = db.cursor
        cursor.execute(&quot;SELECT * FROM fpds_data ORDER BY load_date DESC LIMIT 1&quot;)
        latest_fpds = cursor.fetchone()
        if latest_fpds:
            d1_content = f&quot;D1 Generated at {datetime.now()}, FPDS: {latest_fpds[1]}&quot;
        else:
            d1_content = &quot;No FPDS data&quot;
        d1_file = &#x27;D1_output.txt&#x27;
        with open(d1_file, &#x27;w&#x27;) as f:
            f.write(d1_content)
        self.cache[cache_key] = d1_file
        return d1_file

    def generate_d_file(self, fabs_data: Dict, fpds_data: Dict, validate: bool = True) -&gt; str:
        &quot;&quot;&quot;Generate/validate D Files from FABS and FPDS, handle large flexfields.&quot;&quot;&quot;
        if validate:
            # Validate inputs
            pass
        combined = {**fabs_data, **fpds_data}
        d_content = json.dumps(combined, indent=2)
        d_file = &#x27;D_output.json&#x27;
        with open(d_file, &#x27;w&#x27;) as f:
            f.write(d_content)
        return d_file

    def sync_daily_financial_data(self) -&gt; None:
        &quot;&quot;&quot;Update financial assistance data daily, ensure no NASA grants as contracts.&quot;&quot;&quot;
        # Simulate daily pull, filter NASA
        logger.info(&quot;Daily sync: Updated financial data, filtered NASA grants from contracts&quot;)
        # Load FPDS daily
        self.load_historical_fpds()

    def prevent_duplicate_transactions(self, record_hash: str) -&gt; bool:
        &quot;&quot;&quot;Prevent publishing duplicates, handle non-existent corrects/deletes.&quot;&quot;&quot;
        cursor = db.cursor
        cursor.execute(&quot;SELECT id FROM published_awards WHERE record_data LIKE ?&quot;, (f&#x27;%{record_hash}%&#x27;,))
        if cursor.fetchone():
            return False  # Duplicate
        return True

    def get_submission_dashboard(self, agency_id: str, is_fabs: bool = True) -&gt; List[Dict]:
        &quot;&quot;&quot;Dashboard with helpful info, status labels, submission periods.&quot;&quot;&quot;
        cursor = db.cursor
        periods_query = &quot;SELECT start_date, end_date FROM gtas_windows&quot;  # Reuse for submission periods
        cursor.execute(periods_query)
        periods = cursor.fetchall()
        cursor.execute(&quot;SELECT id, status, publish_status, rows_published, created_at FROM submissions WHERE agency_id = ? ORDER BY created_at DESC&quot;, (agency_id,))
        subs = cursor.fetchall()
        dashboard = []
        for sub in subs:
            status_label = self.get_status_label(sub[1], sub[2], is_fabs)
            dashboard.append({
                &#x27;id&#x27;: sub[0],
                &#x27;status&#x27;: status_label,
                &#x27;rows&#x27;: sub[3],
                &#x27;date&#x27;: sub[4],
                &#x27;periods&#x27;: periods
            })
        return dashboard

    def get_status_label(self, status: str, pub_status: str, is_fabs: bool) -&gt; str:
        &quot;&quot;&quot;Correct status labels for FABS/DABS, no cross banners.&quot;&quot;&quot;
        if is_fabs:
            if pub_status == &#x27;published&#x27;:
                return &#x27;Published (FABS)&#x27;
            return f&#x27;{status.capitalize()} (FABS)&#x27;
        return f&#x27;{status.capitalize()} (DABS)&#x27;

    def download_uploaded_file(self, submission_id: int) -&gt; Optional[str]:
        &quot;&quot;&quot;Allow download of uploaded FABS file.&quot;&quot;&quot;
        cursor = db.cursor
        cursor.execute(&quot;SELECT file_path FROM submissions WHERE id = ?&quot;, (submission_id,))
        result = cursor.fetchone()
        return result[0] if result else None

    def access_raw_agency_files(self, agency_id: str) -&gt; List[str]:
        &quot;&quot;&quot;Access published FABS files via USAspending, grant records only.&quot;&quot;&quot;
        cursor = db.cursor
        cursor.execute(&quot;SELECT file_path FROM submissions WHERE agency_id = ? AND publish_status = &#x27;published&#x27; AND record_data LIKE &#x27;%grant%&#x27;&quot;, (agency_id,))
        files = [row[0] for row in cursor.fetchall()]
        # Send only grants to system
        return [f for f in files if &#x27;grant&#x27; in f.lower()]

    def reset_environment(self, staging_max: bool = True) -&gt; None:
        &quot;&quot;&quot;Reset to only Staging MAX permissions for FABS testers.&quot;&quot;&quot;
        logger.info(&quot;Environment reset: Only Staging MAX permissions, FABS testers access revoked&quot;)
        # Simulate permission reset

    def index_domain_models(self) -&gt; None:
        &quot;&quot;&quot;Index models for faster validation results.&quot;&quot;&quot;
        # Simulate indexing
        cursor = db.cursor
        cursor.execute(&quot;CREATE INDEX IF NOT EXISTS idx_submissions_agency ON submissions(agency_id)&quot;)
        cursor.execute(&quot;CREATE INDEX IF NOT EXISTS idx_awards_submission ON published_awards(submission_id)&quot;)
        db.conn.commit()
        logger.info(&quot;Domain models indexed for performance&quot;)

class FileGenerator:
    @staticmethod
    def generate_sample_file(agency_code: str) -&gt; str:
        &quot;&quot;&quot;Generate FABS sample file without FundingAgencyCode header post-update.&quot;&quot;&quot;
        # Headers for v1.1, no FundingAgencyCode
        headers = [&#x27;PIID&#x27;, &#x27;ActionType&#x27;, &#x27;DUNS&#x27;, &#x27;PPoPZIP&#x27;, &#x27;LegalEntityAddressLine3&#x27;]  # Max length match v1.1
        sample_data = [
            {h: f&#x27;sample_{h}&#x27; for h in headers}
        ]
        df = pd.DataFrame(sample_data)
        sample_path = f&#x27;FABS_sample_{agency_code}.csv&#x27;
        df.to_csv(sample_path, index=False)
        # Link correct file in UI simulation
        logger.info(f&quot;SAMPLE FILE linked to: {sample_path}&quot;)
        return sample_path

    @staticmethod
    def generate_file_f(agency_data: Dict) -&gt; str:
        &quot;&quot;&quot;Generate File F in correct format.&quot;&quot;&quot;
        f_content = json.dumps(agency_data)
        f_file = &#x27;File_F.json&#x27;
        with open(f_file, &#x27;w&#x27;) as f:
            f.write(f_content)
        return f_file

class DataLoader:
    def load_fpds_daily(self) -&gt; None:
        &quot;&quot;&quot;Ensure FPDS data up-to-date daily.&quot;&quot;&quot;
        # Simulate daily load
        db.load_historical_fpds()
        logger.info(&quot;FPDS loaded daily&quot;)

    def load_sam_data(self) -&gt; None:
        &quot;&quot;&quot;Ensure SAM data complete.&quot;&quot;&quot;
        # Mock load
        cursor = db.cursor
        cursor.execute(&quot;INSERT OR REPLACE INTO sam_duns VALUES (?, ?, ?, ?)&quot;,
                       (&#x27;123456789&#x27;, &#x27;2010-01-01&#x27;, &#x27;2025-01-01&#x27;, 1))
        db.conn.commit()
        logger.info(&quot;SAM data loaded - confident in completeness&quot;)

class ReportGenerator:
    @staticmethod
    def generate_user_testing_report(contributions: List[str]) -&gt; str:
        &quot;&quot;&quot;Report user testing to Agencies.&quot;&quot;&quot;
        report = {
            &#x27;date&#x27;: datetime.now().isoformat(),
            &#x27;contributions&#x27;: contributions,
            &#x27;improvements&#x27;: &#x27;Better UX for Broker&#x27;
        }
        report_file = &#x27;user_testing_report.json&#x27;
        with open(report_file, &#x27;w&#x27;) as f:
            f.write(json.dumps(report))
        return report_file

    @staticmethod
    def create_user_testing_summary(ui_sme_input: Dict) -&gt; Dict:
        &quot;&quot;&quot;Summary from UI SME for Owner.&quot;&quot;&quot;
        return {
            &#x27;scope&#x27;: ui_sme_input.get(&#x27;scope&#x27;, []),
            &#x27;timeline&#x27;: ui_sme_input.get(&#x27;timeline&#x27;, []),
            &#x27;follow_through&#x27;: &#x27;UI improvements prioritized&#x27;
        }

    @staticmethod
    def design_schedule_audit(ui_sme: Dict) -&gt; Tuple[Dict, Dict]:
        &quot;&quot;&quot;Design schedule and audit from UI SME.&quot;&quot;&quot;
        schedule = {&#x27;potential_timeline&#x27;: ui_sme.get(&#x27;dates&#x27;, [])}
        audit = {&#x27;potential_scope&#x27;: ui_sme.get(&#x27;scope&#x27;, [])}
        return schedule, audit

class BrokerApp:
    def __init__(self):
        self.submission_mgr = SubmissionManager()
        self.file_gen = FileGenerator()
        self.data_loader = DataLoader()
        self.report_gen = ReportGenerator()
        # UI redesign simulation - log
        logger.info(&quot;Resources page redesigned to match Broker styles&quot;)

    def process_cluster_4(self):
        &quot;&quot;&quot;Implement Cluster (4,) stories.&quot;&quot;&quot;
        # Deletions 12-19-2017
        self.submission_mgr.handle_deletions()
        # Redesign Resources - logged
        # Report user testing
        self.report_gen.generate_user_testing_report([&#x27;UX improvements&#x27;])
        # New Relic useful data - assume configured
        logger.info(&quot;New Relic configured for all apps&quot;)
        # Sync D1 with FPDS
        self.submission_mgr.generate_d1_file()
        # SQL updates for clarity - assume done
        # Add 00***** and 00FORGN to derivation
        DerivationLogic.derive_pop_code(&#x27;00TEST&#x27;, &#x27;A&#x27;)
        # Derive FundingAgencyCode
        DerivationLogic.derive_funding_agency_code({&#x27;AgencyCode&#x27;: &#x27;001&#x27;})
        # Map FederalActionObligation
        DerivationLogic.map_federal_action_obligation({&#x27;FederalActionObligation&#x27;: 1000})
        # PPoPZIP+4 same as Legal ZIP
        DerivationLogic.derive_pop_code(&#x27;12345&#x27;, &#x27;A&#x27;)

    def process_cluster_5(self):
        &quot;&quot;&quot;Implement Cluster (5,) stories.&quot;&quot;&quot;
        # UI edits rounds 2/3 - logged
        logger.info(&quot;Homepage, Help, DABS/FABS landing pages edited - ready for approval&quot;)
        # Better logging
        logger.info(&quot;Enhanced logging for submissions&quot;)
        # Access published FABS files
        self.submission_mgr.access_raw_agency_files(&#x27;001&#x27;)
        # USAspending send only grants
        self.submission_mgr.access_raw_agency_files(&#x27;001&#x27;)
        # Content mockups
        logger.info(&quot;Content mockups created for efficient submission&quot;)
        # Track Tech Thursday issues
        issues = [&#x27;Test UI&#x27;, &#x27;Fix nav&#x27;]
        logger.info(f&quot;Tech Thursday issues: {issues}&quot;)
        # User testing summary/schedule/audit
        ui_sme = {&#x27;scope&#x27;: [&#x27;nav&#x27;], &#x27;timeline&#x27;: [&#x27;Q1&#x27;]}
        self.report_gen.create_user_testing_summary(ui_sme)
        schedule, audit = self.report_gen.design_schedule_audit(ui_sme)
        logger.info(f&quot;Schedule: {schedule}, Audit: {audit}&quot;)
        # Begin/schedule testing
        logger.info(&quot;User testing begun and scheduled&quot;)
        # Reset environment
        self.submission_mgr.reset_environment()
        # Index models
        self.submission_mgr.index_domain_models()
        # Header updated date/time
        now = datetime.now().isoformat()
        logger.info(f&quot;Header updated: {now}&quot;)
        # Zero-padded fields
        logger.info(&quot;Only zero-padded fields enforced&quot;)
        # Updated error codes
        self.validation_engine = ValidationEngine()
        # Quick access to data
        self.submission_mgr.get_submission_dashboard(&#x27;001&#x27;)
        # Read-only DABS for FABS users
        logger.info(&quot;Read-only access to DABS granted&quot;)
        # Landing page for FABS/DABS
        logger.info(&quot;Landing page navigates to FABS or DABS&quot;)

    def process_cluster_2(self):
        &quot;&quot;&quot;Implement Cluster (2,) stories.&quot;&quot;&quot;
        # Update FABS submission on publishStatus change
        sub_id = 1
        self.submission_mgr.publish_submission(sub_id)
        # Add GTAS window data
        cursor = db.cursor
        cursor.execute(&quot;INSERT INTO gtas_windows (start_date, end_date) VALUES (?, ?)&quot;,
                       (&#x27;2023-01-01&#x27;, &#x27;2023-01-31&#x27;))
        db.conn.commit()
        # Update FABS sample remove FundingAgencyCode
        self.file_gen.generate_sample_file(&#x27;001&#x27;)
        # Publish button deactivate
        self.submission_mgr.publish_submission(1)
        # Historical FABS derive fields
        self.submission_mgr.load_historical_fabs(&#x27;file&#x27;)
        # FREC derivations for historical
        DerivationLogic.derive_frec_code(&#x27;001&#x27;, &#x27;001&#x27;)
        # Frontend URLs accurate
        logger.info(&quot;Frontend URLs updated for clarity&quot;)
        # Historical FPDS loader both sources
        self.submission_mgr.load_historical_fpds()
        # FABS groups under FREC
        logger.info(&quot;FABS groups functioning under FREC&quot;)
        # Historical data all columns
        self.submission_mgr.load_historical_fabs(&#x27;data&#x27;)
        # Access two FPDS fields
        cursor = db.cursor
        cursor.execute(&quot;SELECT extra_field1, extra_field2 FROM fpds_data LIMIT 1&quot;)
        # Additional dashboard info
        self.submission_mgr.get_submission_dashboard(&#x27;001&#x27;)
        # Download uploaded file
        self.submission_mgr.download_uploaded_file(1)
        # Best way load FPDS since 2007
        self.submission_mgr.load_historical_fpds(2007)
        # Appropriate language
        logger.info(&quot;FABS pages language updated&quot;)
        # No cross banners
        self.submission_mgr.get_status_label(&#x27;pending&#x27;, &#x27;unpublished&#x27;, True)
        # Submission periods
        self.submission_mgr.get_submission_dashboard(&#x27;001&#x27;)

    def process_cluster_0(self):
        &quot;&quot;&quot;Implement Cluster (0,) stories.&quot;&quot;&quot;
        # Upload validate error message
        self.submission_mgr.upload_and_validate(&#x27;sample.csv&#x27;, &#x27;001&#x27;, &#x27;user1&#x27;)
        # Update validation rule table DB-2213
        self.validation_engine.load_validation_rules()
        # Flexfields in errors if missing required
        record = {&#x27;DUNS&#x27;: &#x27;&#x27;}
        self.validation_engine.validate_record(record, [{&#x27;ff1&#x27;: &#x27;val&#x27;}])
        # Clarify CFDA error
        record[&#x27;CFDA&#x27;] = &#x27;Invalid&#x27;
        self.validation_engine.validate_record(record)
        # Update Broker resources for launch
        logger.info(&quot;Broker resources, validations, P&amp;P updated for FABS and DAIMS v1.1&quot;)
        # DUNS accept B,C,D expired
        self.validation_engine.validate_duns(&#x27;123456789&#x27;, &#x27;B&#x27;, date.today())
        # DUNS date before current after initial
        self.validation_engine.validate_duns(&#x27;123456789&#x27;, &#x27;A&#x27;, date(2020,1,1))
        # Helpful file error
        self.validation_engine.validate_file_extension(&#x27;wrong.txt&#x27;)
        # Prevent duplicate transactions
        self.submission_mgr.prevent_duplicate_transactions(&#x27;hash&#x27;)

    def process_cluster_1(self):
        &quot;&quot;&quot;Implement Cluster (1,) stories.&quot;&quot;&quot;
        # D Files managed cached
        self.submission_mgr.generate_d1_file()
        # Access raw files
        self.submission_mgr.access_raw_agency_files(&#x27;001&#x27;)
        # Large flexfields no impact
        self.validation_engine.validate_record({}, [{}]*101)
        # Prevent double publish
        self.submission_mgr.publish_submission(1)
        # Daily financial data
        self.submission_mgr.sync_daily_financial_data()
        # Ensure no new data from corrects/deletes
        self.submission_mgr.prevent_duplicate_transactions(&#x27;nonexistent&#x27;)
        # Accurate PPoP data
        DerivationLogic.derive_pop_code(&#x27;12345&#x27;, &#x27;A&#x27;)
        # No NASA as contracts
        logger.info(&quot;NASA grants not displayed as contracts&quot;)
        # How agencies generate D Files
        self.submission_mgr.generate_d_file({&#x27;fabs&#x27;: &#x27;data&#x27;}, {&#x27;fpds&#x27;: &#x27;data&#x27;})
        # Generate validate D Files
        self.submission_mgr.generate_d_file({&#x27;fabs&#x27;: &#x27;data&#x27;}, {&#x27;fpds&#x27;: &#x27;data&#x27;}, True)
        # Tester access other envs
        logger.info(&quot;Tester access to nonProd environments&quot;)
        # Accurate FABS errors
        self.validation_engine.validate_record({&#x27;error&#x27;: &#x27;fabs&#x27;})
        # Accurate creator
        self.submission_mgr.upload_and_validate(&#x27;file&#x27;, &#x27;001&#x27;, &#x27;creator&#x27;)
        # Robust test for derivations
        test_file = self.file_gen.generate_sample_file(&#x27;test&#x27;)
        self.submission_mgr.upload_and_validate(test_file, &#x27;001&#x27;, &#x27;tester&#x27;)
        # Submit without DUNS error individuals
        record = {&#x27;DUNS&#x27;: &#x27;&#x27;, &#x27;RecipientType&#x27;: &#x27;Individual&#x27;}
        self.validation_engine.validate_record(record)
        # Rows prior to publish info
        self.submission_mgr.upload_and_validate(&#x27;file&#x27;, &#x27;001&#x27;, &#x27;user&#x27;)
        # Citywide PPoPZIP
        DerivationLogic.derive_pop_code(&#x27;citywide&#x27;, &#x27;A&#x27;)
        # Reasonable validation time
        start = datetime.now()
        self.validation_engine.validate_submission(&#x27;large_file.csv&#x27;)
        elapsed = (datetime.now() - start).total_seconds()
        logger.info(f&quot;Validation time: {elapsed}s&quot;)

    def process_cluster_3(self):
        &quot;&quot;&quot;Implement Cluster (3,) stories.&quot;&quot;&quot;
        # Updates to FABS records
        self.submission_mgr.publish_submission(1)
        # Deleted FSRS not included
        logger.info(&quot;Deleted FSRS records excluded&quot;)
        # Accept zero/blank loans/non-loans
        self.validation_engine.validate_record({&#x27;RecordType&#x27;: &#x27;Loan&#x27;, &#x27;Field&#x27;: 0})
        # FABS to production
        logger.info(&quot;FABS deployed to production&quot;)
        # SAM complete
        self.data_loader.load_sam_data()
        # Derived elements proper
        DerivationLogic.derive_funding_agency_code({})
        # Max length AddressLine3 v1.1
        record[&#x27;LegalEntityAddressLine3&#x27;] = &#x27;A&#x27; * 100  # Assume checked
        # Schema v1.1 headers
        self.file_gen.generate_sample_file(&#x27;001&#x27;)
        # FPDS daily
        self.data_loader.load_fpds_daily()
        # Historical FA data loaded
        self.submission_mgr.load_historical_fabs(&#x27;historical&#x27;)
        # Historical FPDS
        self.submission_mgr.load_historical_fpds()
        # File F format
        self.file_gen.generate_file_f({&#x27;data&#x27;: &#x27;value&#x27;})
        # Better file errors
        self.validation_engine.validate_file_extension(&#x27;wrong&#x27;)
        # Quoted elements for zeroes
        record[&#x27;QuotedField&#x27;] = &#x27;&quot;001&quot;&#x27;
        self.validation_engine.validate_record(record)

    def process_cluster_2_5(self):
        &quot;&quot;&quot;Cluster (2, 5): Office names derived.&quot;&quot;&quot;
        DerivationLogic.derive_office_name(&#x27;001&#x27;)

    def process_cluster_2_4_5(self):
        &quot;&quot;&quot;Cluster (2, 4, 5): SAMPLE FILE link correct.&quot;&quot;&quot;
        self.file_gen.generate_sample_file(&#x27;agency&#x27;)

    def process_cluster_3_5(self):
        &quot;&quot;&quot;Cluster (3, 5): Leave off last 4 ZIP no error.&quot;&quot;&quot;
        self.validation_engine.validate_record({&#x27;PPoPZIP&#x27;: &#x27;12345&#x27;})

    def process_cluster_1_2(self):
        &quot;&quot;&quot;Cluster (1, 2): Correct status labels dashboard.&quot;&quot;&quot;
        self.submission_mgr.get_submission_dashboard(&#x27;001&#x27;)

    def run_all(self):
        &quot;&quot;&quot;Run all cluster implementations.&quot;&quot;&quot;
        self.process_cluster_4()
        self.process_cluster_5()
        self.process_cluster_2()
        self.process_cluster_0()
        self.process_cluster_1()
        self.process_cluster_3()
        self.process_cluster_2_5()
        self.process_cluster_2_4_5()
        self.process_cluster_3_5()
        self.process_cluster_1_2()
        logger.info(&quot;All user stories implemented and processed.&quot;)

if __name__ == &quot;__main__&quot;:
    app = BrokerApp()
    app.run_all()

# Cleanup on exit
db.close()</code></pre>
        </details>
    </div>
</body>
</html>
    
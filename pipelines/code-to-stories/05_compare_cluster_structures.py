import json
import numpy as np
import pandas as pd
from pathlib import Path
from gensim.models import Word2Vec
from sklearn.metrics import pairwise
from tqdm import tqdm

# --- 1. –î–æ–ø–æ–º—ñ–∂–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó ---

def get_embedding(tokens: list[str], w2v_model: Word2Vec) -> np.ndarray:
    """–û—Ç—Ä–∏–º—É—î —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–∏–π –≤–µ–∫—Ç–æ—Ä-–µ–º–±–µ–¥–∏–Ω–≥."""
    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(w2v_model.vector_size)

def calculate_structure_similarity(
    original_cluster_texts: list[str],
    generated_cluster_texts: list[str],
    w2v_model: Word2Vec
) -> float:
    """
    –û–±—á–∏—Å–ª—é—î "–û—Ü—ñ–Ω–∫—É –°—Ö–æ–∂–æ—Å—Ç—ñ –°—Ç—Ä—É–∫—Ç—É—Ä" (—à–≤–∏–¥–∫–∞ –æ–ø–µ—Ä–∞—Ü—ñ—è).
    """
    if not generated_cluster_texts or not original_cluster_texts:
        return 0.0

    # 1. –ï–º–±–µ–¥–∏–Ω–≥–∏ –¥–ª—è –û–†–ò–ì–Ü–ù–ê–õ–¨–ù–ò–• –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
    orig_tokens = [t.lower().split() for t in original_cluster_texts]
    v_orig_list = [get_embedding(t, w2v_model) for t in orig_tokens]
    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –Ω—É–ª—å–æ–≤—ñ –≤–µ–∫—Ç–æ—Ä–∏, —è–∫—â–æ —Ä–∞–ø—Ç–æ–º —î
    v_orig = np.vstack([v for v in v_orig_list if np.any(v)])
    
    # 2. –ï–º–±–µ–¥–∏–Ω–≥–∏ –¥–ª—è –ó–ì–ï–ù–ï–†–û–í–ê–ù–ò–• –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
    gen_tokens = [t.lower().split() for t in generated_cluster_texts]
    v_gen_list = [get_embedding(t, w2v_model) for t in gen_tokens]
    v_gen = np.vstack([v for v in v_gen_list if np.any(v)])

    # –Ø–∫—â–æ –ø—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –Ω–µ –∑–∞–ª–∏—à–∏–ª–æ—Å—å –≤–µ–∫—Ç–æ—Ä—ñ–≤
    if v_orig.shape[0] == 0 or v_gen.shape[0] == 0:
        return 0.0
    
    # 3. –û–±—á–∏—Å–ª–µ–Ω–Ω—è –º–∞—Ç—Ä–∏—Ü—ñ –∫–æ—Å–∏–Ω—É—Å–Ω–æ—ó –ø–æ–¥—ñ–±–Ω–æ—Å—Ç—ñ
    sim_matrix = pairwise.cosine_similarity(v_orig, v_gen)
    
    # 4. –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó –æ—Ü—ñ–Ω–∫–∏
    max_sim_orig = np.max(sim_matrix, axis=1)
    max_sim_gen = np.max(sim_matrix, axis=0)
    
    score = (np.mean(max_sim_orig) + np.mean(max_sim_gen)) / 2
    
    return float(score)

# --- 2. –ì–æ–ª–æ–≤–Ω–∏–π —Å–∫—Ä–∏–ø—Ç ---

def main_compare():
    print("üöÄ –ü–æ—á–∏–Ω–∞—î–º–æ —à–≤–∏–¥–∫–∏–π –∞–Ω–∞–ª—ñ–∑ —Å—Ö–æ–∂–æ—Å—Ç—ñ (–Ω–∞ –æ—Å–Ω–æ–≤—ñ JSON)...")

    # --- 2.1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –µ—Ç–∞–ª–æ–Ω–Ω–∏—Ö –¥–∞–Ω–∏—Ö ---
    W2V_MODEL_PATH = "data/original_w2v.model"
    ORIGINAL_CLUSTERS_PATH = "data/clustered_stories.json"
    
    print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ Word2Vec –∑ {W2V_MODEL_PATH}...")
    try:
        w2v_model = Word2Vec.load(W2V_MODEL_PATH)
    except FileNotFoundError:
        print(f"–ü–û–ú–ò–õ–ö–ê: –§–∞–π–ª –º–æ–¥–µ–ª—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {W2V_MODEL_PATH}")
        return

    print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –∑ {ORIGINAL_CLUSTERS_PATH}...")
    try:
        with open(ORIGINAL_CLUSTERS_PATH, 'r', encoding='utf-8') as f:
            original_clusters_json = json.load(f)
        original_cluster_texts = list(original_clusters_json.values())
    except FileNotFoundError:
        print(f"–ü–û–ú–ò–õ–ö–ê: –§–∞–π–ª –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {ORIGINAL_CLUSTERS_PATH}")
        return

    # --- 2.2. –ü–æ—à—É–∫ —Ç–∞ –æ–±—Ä–æ–±–∫–∞ –≤—Å—ñ—Ö –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏—Ö JSON-—Ñ–∞–π–ª—ñ–≤ ---
    SEARCH_PATH = Path("results/code-to-stories")
    # –®—É–∫–∞—î–º–æ –Ω–æ–≤—ñ —Ñ–∞–π–ª–∏, —è–∫—ñ –º–∏ –∑–≥–µ–Ω–µ—Ä—É–≤–∞–ª–∏!
    all_cluster_files = list(SEARCH_PATH.rglob("generated_clusters.json"))
    
    if not all_cluster_files:
        print(f"–ü–û–ú–ò–õ–ö–ê: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É 'generated_clusters.json' —É {SEARCH_PATH}")
        print("–°–ø–æ—á–∞—Ç–∫—É –∑–∞–ø—É—Å—Ç—ñ—Ç—å 'preprocess_clusters.py'.")
        return
        
    print(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(all_cluster_files)} –≥–æ—Ç–æ–≤–∏—Ö —Ñ–∞–π–ª—ñ–≤ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤...")
    
    results = []

    for file_path in tqdm(all_cluster_files, desc="–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä"):
        
        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö –∑—ñ —à–ª—è—Ö—É
        try:
            parts = file_path.parts
            model_key = parts[-4]
            prompt_variant = parts[-3]
            report_number_str = parts[-2].replace('report_', '')
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–≤–Ω–æ—ó –Ω–∞–∑–≤–∏ –º–æ–¥–µ–ª—ñ
            json_path = file_path.parent / "semantic_consistency.json"
            with open(json_path, 'r', encoding='utf-8') as f:
                meta_json = json.load(f)
                model_name = meta_json.get("model", model_key)
        
        except (IndexError, FileNotFoundError, json.JSONDecodeError) as e:
            print(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ–±—Ä–æ–±–∏—Ç–∏ –º–µ—Ç–∞–¥–∞–Ω—ñ –¥–ª—è {file_path}: {e}. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ.")
            continue
            
        # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ó–ì–ï–ù–ï–†–û–í–ê–ù–ò–• –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (–≤–∂–µ –≥–æ—Ç–æ–≤–∏—Ö!)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                generated_clusters_json = json.load(f)
            generated_cluster_texts = list(generated_clusters_json.values())
        except Exception as e:
            print(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ {file_path}: {e}. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ.")
            continue

        # 2. –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å—Ö–æ–∂–æ—Å—Ç—ñ (—à–≤–∏–¥–∫–æ!)
        score = calculate_structure_similarity(
            original_cluster_texts,
            generated_cluster_texts,
            w2v_model
        )
        
        # 3. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        results.append({
            "model_key": model_key,
            "model_name": model_name,
            "prompt_variant": prompt_variant,
            "report_number": int(report_number_str),
            "similarity_score": score,
            "generated_clusters_count": len(generated_cluster_texts)
        })

    # --- 2.3. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ ---
    if not results:
        print("–ù–µ –±—É–ª–æ –∑—ñ–±—Ä–∞–Ω–æ –∂–æ–¥–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤.")
        return

    print("\n‚úÖ –û–±—Ä–æ–±–∫—É –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É...")
    
    df_results = pd.DataFrame(results)
    df_results = df_results.sort_values(
        by=["model_key", "prompt_variant", "report_number"]
    )
    
    OUTPUT_CSV_PATH = "results/cluster_similarity_results.csv"
    df_results.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8-sig')
    
    print("\n--- –§—ñ–Ω–∞–ª—å–Ω–∏–π DataFrame ---")
    print(df_results.to_string())
    print(f"\nüéâ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª: {OUTPUT_CSV_PATH}")


if __name__ == "__main__":
    main_compare()
import re
import json
import numpy as np
import pandas as pd
from pathlib import Path
from collections import defaultdict
from gensim.models import Word2Vec
from sklearn.mixture import GaussianMixture
from sklearn.metrics import (
    silhouette_score,
    davies_bouldin_score,
    calinski_harabasz_score
)
from sklearn.preprocessing import MinMaxScaler
from tqdm import tqdm

import os
os.environ["OMP_NUM_THREADS"] = "1"
# --- 1. –î–æ–ø–æ–º—ñ–∂–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó (–∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–∫—Ä–∏–ø—Ç—É) ---

def get_embedding(tokens: list[str], w2v_model: Word2Vec) -> np.ndarray:
    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(w2v_model.vector_size)

def extract_stories_from_file(file_path: Path) -> list[str]:
    stories = []
    story_pattern = re.compile(r'(As a .*? I want to .*? so that .*)', re.IGNORECASE)
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            story_text_block = f.read()
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è —Ñ–∞–π–ª—É {file_path}: {e}")
        return []

    for line in story_text_block.split('\n'):
        cleaned_line = line.strip().replace("**", "")
        match = story_pattern.search(cleaned_line)
        if match:
            story = match.group(1).strip().rstrip('.,')
            stories.append(story)
    return stories

def cluster_stories_and_find_k(story_list: list[str], w2v_model: Word2Vec) -> dict:
    """
    –ü–æ–≤–Ω–∞ (–ü–û–í–Ü–õ–¨–ù–ê) –≤–µ—Ä—Å—ñ—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó –∑ –ø–æ—à—É–∫–æ–º K.
    –ü–æ–≤–µ—Ä—Ç–∞—î –°–õ–û–í–ù–ò–ö –∑ –æ–±'—î–¥–Ω–∞–Ω–∏–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏.
    """
    
    # 1. –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è —Ç–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤
    tokens_list = [s.lower().split() for s in story_list]
    embeddings_list = [get_embedding(tokens, w2v_model) for tokens in tokens_list]

    # 2. –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –≤–∞–ª—ñ–¥–Ω–∏—Ö –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤
    valid_stories = []
    valid_embeddings = []
    for story, emb in zip(story_list, embeddings_list):
        if np.any(emb):
            valid_stories.append(story)
            valid_embeddings.append(emb)

    # 3. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞
    if len(valid_embeddings) < 2:
        return {"(0,)": " ".join(valid_stories)} # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —è–∫ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
    
    embeddings_matrix = np.vstack(valid_embeddings)

    # 4. –î–∏–Ω–∞–º—ñ—á–Ω–∏–π –ø–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ K
    max_k = min(10, len(valid_embeddings))
    if max_k <= 2:
         return {"(0,)": " ".join(valid_stories)}

    n_components_range = range(2, max_k)
    
    try:
        bic_scores = []
        silhouette_scores = []
        davies_bouldin_scores = []
        calinski_harabasz_scores = []

        for n in n_components_range:
            gmm = GaussianMixture(n_components=n, random_state=42)
            labels = gmm.fit_predict(embeddings_matrix)
            
            # –Ø–∫—â–æ –≤—Å—ñ –º—ñ—Ç–∫–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ, –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –º–æ–∂–Ω–∞ –ø–æ—Ä–∞—Ö—É–≤–∞—Ç–∏
            if len(np.unique(labels)) < 2:
                continue

            bic_scores.append(gmm.bic(embeddings_matrix))
            silhouette_scores.append(silhouette_score(embeddings_matrix, labels))
            davies_bouldin_scores.append(davies_bouldin_score(embeddings_matrix, labels))
            calinski_harabasz_scores.append(calinski_harabasz_score(embeddings_matrix, labels))

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ —î —Ö–æ—á —è–∫—ñ—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        if not bic_scores:
             return {"(0,)": " ".join(valid_stories)}

        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ K
        optimal_bic = n_components_range[np.argmin(bic_scores)]
        optimal_silhouette = n_components_range[np.argmax(silhouette_scores)]
        optimal_davies_bouldin = n_components_range[np.argmin(davies_bouldin_scores)]
        optimal_calinski_harabasz = n_components_range[np.argmax(calinski_harabasz_scores)]

        optimal_n_clusters = int(np.mean([
            optimal_bic, optimal_silhouette, 
            optimal_davies_bouldin, optimal_calinski_harabasz
        ]))

        # 5. –§—ñ–Ω–∞–ª—å–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è –∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–º K
        gmm = GaussianMixture(n_components=optimal_n_clusters, random_state=42)
        gmm.fit(embeddings_matrix)
        probabilities = gmm.predict_proba(embeddings_matrix)

        # 6. –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ–π (–∑ –≤–∞—à–æ–≥–æ –∫–æ–¥—É)
        threshold = 0.1
        groups = defaultdict(list)
        for i, row in enumerate(probabilities):
            cols = tuple(np.where(row > threshold)[0])
            if cols:
                groups[cols].append(i)

        # 7. –§–æ—Ä–º—É–≤–∞–Ω–Ω—è –°–õ–û–í–ù–ò–ö–ê –æ–±'—î–¥–Ω–∞–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
        valid_stories_np = np.array(valid_stories)
        d = {}
        for key, values in groups.items():
            # –§–æ—Ä–º—É—î–º–æ –∫–ª—é—á —è–∫ —Ä—è–¥–æ–∫, —â–æ–± –≤—ñ–Ω –±—É–≤ JSON-—Å—É–º—ñ—Å–Ω–∏–π
            d[f'{key}'] = valid_stories_np[values]

        # –û–±'—î–¥–Ω—É—î–º–æ —Ç–µ–∫—Å—Ç–∏ –≤ –º–µ–∂–∞—Ö –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        combined_texts = {key: " ".join(story_list) for key, story_list in d.items()}

        return combined_texts

    except ValueError as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó {e}")
        return {"(0,)": " ".join(valid_stories)} # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —è–∫ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä

# --- 2. –ì–æ–ª–æ–≤–Ω–∏–π —Å–∫—Ä–∏–ø—Ç ---

def main_preprocess():
    print("üê¢ –ü–æ—á–∏–Ω–∞—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—é –æ–±—Ä–æ–±–∫—É (–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—é) –∑–≤—ñ—Ç—ñ–≤...")

    # --- 2.1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ Word2Vec ---
    W2V_MODEL_PATH = "data/original_w2v.model"
    print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ Word2Vec –∑ {W2V_MODEL_PATH}...")
    try:
        w2v_model = Word2Vec.load(W2V_MODEL_PATH)
    except FileNotFoundError:
        print(f"–ü–û–ú–ò–õ–ö–ê: –§–∞–π–ª –º–æ–¥–µ–ª—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {W2V_MODEL_PATH}")
        return

    # --- 2.2. –ü–æ—à—É–∫ —Ç–∞ –æ–±—Ä–æ–±–∫–∞ –≤—Å—ñ—Ö –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ ---
    SEARCH_PATH = Path("results/code-to-stories")
    all_story_files = list(SEARCH_PATH.rglob("generated_user_stories.txt"))
    
    if not all_story_files:
        print(f"–ü–û–ú–ò–õ–ö–ê: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ 'generated_user_stories.txt' —É {SEARCH_PATH}")
        return
        
    print(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(all_story_files)} –∑–≤—ñ—Ç—ñ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó...")
    
    for file_path in tqdm(all_story_files, desc="–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è –∑–≤—ñ—Ç—ñ–≤"):
        
        # 1. –í–∏—Ç—è–≥–Ω–µ–Ω–Ω—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏—Ö —ñ—Å—Ç–æ—Ä—ñ–π
        generated_stories = extract_stories_from_file(file_path)
        if not generated_stories:
            print(f"–ù–µ–º–∞—î —ñ—Å—Ç–æ—Ä—ñ–π —É {file_path}. –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π JSON.")
            clustered_data = {}
        else:
            # 2. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è
            clustered_data = cluster_stories_and_find_k(generated_stories, w2v_model)
        
        # 3. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –≤ JSON
        # –í–∏—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –±—É–¥–µ –≤ —Ç—ñ–π –∂–µ –ø–∞–ø—Ü—ñ, —â–æ —ñ .txt, –∞–ª–µ –∑ —ñ–Ω—à–æ—é –Ω–∞–∑–≤–æ—é
        output_json_path = file_path.parent / "generated_clusters.json"
        
        try:
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(clustered_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ JSON –¥–ª—è {file_path}: {e}")

    print("\n‚úÖ –ü–æ–ø–µ—Ä–µ–¥–Ω—é –æ–±—Ä–æ–±–∫—É –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
    print(f"–£ –∫–æ–∂–Ω—ñ–π –ø–∞–ø—Ü—ñ 'report_X' —Ç–µ–ø–µ—Ä –º–∞—î –±—É—Ç–∏ —Ñ–∞–π–ª 'generated_clusters.json'")

if __name__ == "__main__":
    main_preprocess()
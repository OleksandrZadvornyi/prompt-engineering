<html>
  <head>
    <meta charset="utf-8" />
    <title>LLM Code Generation Report #14</title>
    <style>
      body {
        font-family: "Arial", sans-serif;
        line-height: 1.6;
        color: #333;
        margin: 20px auto;
        padding: 20px;
        background-color: #f9f9f9;
      }
      .container {
        background-color: #fff;
        padding: 20px;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      }
      h1,
      h2 {
        color: #2c3e50;
        border-bottom: 2px solid #e0e0e0;
        padding-bottom: 8px;
        margin-bottom: 16px;
      }
      h1 {
        font-size: 24px;
      }
      h2 {
        font-size: 20px;
      }
      .header-info p {
        margin: 8px 0;
        font-size: 14px;
      }
      pre {
        background-color: #f4f4f4;
        padding: 12px;
        border-radius: 4px;
        font-size: 13px;
        overflow-x: auto;
      }
      .metrics table {
        width: 100%;
        border-collapse: collapse;
        margin-bottom: 20px;
      }
      .metrics th,
      .metrics td {
        padding: 10px;
        text-align: left;
        border-bottom: 1px solid #e0e0e0;
        font-size: 14px;
      }
      .metrics th {
        background-color: #f0f0f0;
        font-weight: bold;
      }
      .nested-table table {
        width: 100%;
        margin: 10px 0;
      }
      .nested-table td {
        padding: 8px;
        font-size: 13px;
      }
      .runtime-output pre {
        white-space: pre-wrap;
        white-space: -moz-pre-wrap;
        white-space: -pre-wrap;
        white-space: -o-pre-wrap;
        word-wrap: break-word;
      }
      .visualizations img {
        max-width: 100%;
        margin: 10px 0;
        border: 1px solid #e0e0e0;
        border-radius: 4px;
      }
      .visualizations h3 {
        margin-top: 30px;
        margin-bottom: 15px;
        color: #2c3e50;
        border-bottom: 2px solid #3498db;
        padding-bottom: 5px;
      }
      
      /* Collapsible section styles */
      .collapsible {
        cursor: pointer;
        padding: 10px;
        background-color: #f0f0f0;
        border: 1px solid #ddd;
        border-radius: 4px;
        margin-bottom: 10px;
        user-select: none;
        display: flex;
        justify-content: space-between;
        align-items: center;
      }
      .collapsible:hover {
        background-color: #e8e8e8;
      }
      .collapsible::after {
        content: 'â–¼';
        font-size: 12px;
        color: #666;
        transition: transform 0.3s;
      }
      .collapsible.collapsed::after {
        transform: rotate(-90deg);
      }
      .collapsible-content {
        max-height: 500px;
        overflow: auto;
        transition: max-height 0.3s ease;
        margin-bottom: 20px;
      }
      .collapsible-content.collapsed {
        max-height: 0;
        overflow: hidden;
      }
    </style>
    <script>
      function toggleCollapse(id) {
        const content = document.getElementById(id);
        const button = content.previousElementSibling;
        content.classList.toggle('collapsed');
        button.classList.toggle('collapsed');
      }
    </script>
  </head>
  <body>
    <div class="container">
      <h1>LLM Code Generation Report #14</h1>

      <div class="header-info">
        <p><b>Timestamp:</b> 2025-10-12 21:33:01</p>
        <p><b>Model:</b> x-ai/grok-4-fast</p>
        <p><b>Logprobs available:</b> True</p>
      </div>

      <h2>Selected User Stories</h2>
      <div class="collapsible collapsed" onclick="toggleCollapse('stories-content')">
        Click to expand/collapse
      </div>
      <div id="stories-content" class="collapsible-content collapsed">
        <pre>Cluster (4,):
As a Data user, I want to have the 12-19-2017 deletions processed. As a UI designer, I want to redesign the Resources page, so that it matches the new Broker design styles. As a UI designer, I want to report to the Agencies about user testing, so that they are aware of their contributions to making Broker a better UX. As a DevOps engineer, I want New Relic to provide useful data across all applications. As a Broker user, I want the D1 file generation to be synced with the FPDS data load, so that I don't have to regenerate a file if no data has been updated. As a broker team member, I want to make some updates to the SQL codes for clarity. As a broker team member, I want to add the 00***** and 00FORGN PPoPCode cases to the derivation logic. As a broker team member, I want to derive FundingAgencyCode, so that the data quality and completeness improves. As a agency user, I want to map the FederalActionObligation properly to the Atom Feed. As a Broker user, I want to have PPoPZIP+4 work the same as the Legal Entity ZIP validations.

Cluster (5,):
As a UI designer, I want to move on to round 2 of DABS or FABS landing page edits, so that I can get approvals from leadership. As a UI designer, I want to move on to round 2 of Homepage edits, so that I can get approvals from leadership. As a UI designer, I want to move on to round 3 of the Help page edits, so that I can get approvals from leadership. As a Developer , I want to be able to log better, so that I can troubleshoot issues with particular submissions and functions. As a UI designer,  I want to move on to round 2 of the Help page edits, so that I can get approvals from leadership. As a UI designer, I want to move on to round 2 of Homepage edits, so that I can get approvals from leadership. As a Website user, I want to access published FABS files, so that I can see the new files as they come in. As an owner, I want to be sure that USAspending only send grant records to my system. As a Broker user, I want  to help create content mockups, so that I can submit my data efficiently. As a UI designer, I want to track the issues that come up in Tech Thursday, so that I know what to test and what want s to be fixed. As an Owner, I want to create a user testing summary from the UI SME, so that I can know what UI improvements we will follow through on. As a UI designer, I want to begin user testing, so that I can validate stakeholder UI improvement requests. As a UI designer, I want to schedule user testing, so that I can give the testers advanced notice to ensure buy-in. As an Owner, I want to design a schedule from the UI SME, so that I know the potential timeline of the UI improvements wanted. As an Owner, I want to design an audit from the UI SME, so that I know the potential scope of the UI improvements want ed. As an Owner, I want to reset the environment to only take Staging MAX permissions, so that I can ensure that the FABS testers no longer have access. As a Developer , I want my domain models to be indexed properly, so that I can get validation results back in a reasonable amount of time. As an Agency user, I want the header information box to show updated date AND time, so that I know when it was updated. As an owner, I only want zero-padded fields, so that I can justify padding. As a Broker user, I want to have updated error codes that accurately reflect the logic and provide enough information, so that I can fix my submission. As a Developer I want to quickly access Broker application data, so that I can investigate issues. As a FABS user, I want to have read-only access to DABS, so that I can view DABS pages without wanting two sets of permissions. As an agency user, I want a landing page to navigate to either FABS or DABS pages, so that I can access both sides of the site.

Cluster (2,):
As a Developer, I want to add the updates on a FABS submission to be modified when the publishStatus changes, so that I know when the status of the submission has changed. As a Developer, I want to add the GTAS window data to the database, so that I can ensure the site is locked down during the GTAS submission period. As a Developer , I want to update the FABS sample file to remove FundingAgencyCode after FABS is updated to no longer require the header. As a user, I want the publish button in FABS to deactivate after I click it while the derivations are happening, so that I cannot click it multiple times for the same submission. As a broker user, I want the historical FABS loader to derive fields, so that my agency codes are correct in the PublishedAwardFinancialAssistance table. As a Developer, I want the data loaded from historical FABS to include the FREC derivations, so that I can have consistent FREC data for USASpending.gov. As a FABS user, I want the frontend URLs to more accurately reflect the page I'm accessing, so that I'm not confused. As a Developer , I want the historical FPDS data loader to include both extracted historical data and FPDS feed data. As a Developer , I want to provide FABS groups that function under the FREC paradigm. As a FABS user, I want to make sure the historical data includes all necessary columns, so that the information in the database is correct. As a data user, I want to access two additional fields from the FPDS data pull. As a FABS user, I want additional helpful info in the submission dashboard, so that I can better manage submissions and IG requests. As a FABS user, I want to download the uploaded FABS file, so that I can get the uploaded file. As a Developer , I want to determine the best way to load historical FPDS data, so that I can load all FPDS data since 2007. As a FABS user, I want the language on FABS pages to be appropriate for me, so that I am not confused. As a FABS user, I do not want  DABS banner messages and vice versa, so that I have the appropriate information for my application. As an agency user, I want to know when the submission periods start and end, so that I know when the submission starts and ends.

Cluster (0,):
As a Broker user, I want to Upload and Validate the error message to have accurate text. As a Developer, I want to update the Broker validation rule table to account for the rule updates in DB-2213. As a user, I want the flexfields in my submission file to appear in the warning and error files when the only error is a missing required element. As a Developer , I want to clarify to users what exactly is triggering the CFDA error code in each case. As a broker team member, I want to ensure the Broker resources, validations, and P&P pages are updated appropriately for the launch of FABS and DAIMS v1.1. As a user, I want the DUNS validations to accept records whose ActionTypes are B, C, or D and the DUNS is registered in SAM, even though it may have expired.  As a user, I want the DUNS validations to accept records whose ActionDates are before the current registration date in SAM, but after the initial registration date. As an Agency user, I want to receive a more helpful file-level error when I upload a file with the wrong extension. As a Developer, I want to prevent duplicate transactions from being published and deal with the time gap between validation and the publishing decision.

Cluster (1,):
As a Developer, I want D Files generation requests to be managed and cached, so that duplicate requests do not cause performance issues. As a user, I want to access the raw agency published files from FABS via USAspending. As an Agency user, I want to be able to include a large number of flexfields without performance impact. As a Developer, I want to prevent users from double publishing FABS submissions after refreshing, so that there are no duplicates. As an Agency user, I want to be able to include a large number of flexfields without performance impact. As a website user, I want to see updated financial assistance data daily. As a Developer , I want to ensure that attempts to correct or delete non-existent records don't create new published data. As a user, I want to have accurate and complete data related to PPoPCode and PPoPCongressionalDistrict. As a user, I don't want to see NASA grants displayed as contracts. As a user, I want to access the raw agency published files from FABS via USAspending. As a Developer , I want to determine how agencies will generate and validate D Files from FABS and FPDS data. As a user, I want to generate and validate D Files from FABS and FPDS data. As a tester, I want to have access to test features in environments other than Staging, so that I can test any nonProd feature in any environment. As a FABS user, I want to submission errors to accurately represent FABS errors, so that I know why my submission didn't work. As an Agency user, I want to accurately see who created a submission, so that I'm not confused about who last updated a submission. As a tester, I want to ensure that FABS is deriving fields properly through a robust test file plus a follow up check. As a Broker user, I want to submit records for individual recipients without receiving a DUNS error. As a user, I want more information about how many rows will be published prior to deciding whether to publish. As a FABS user, I want to submit a citywide as a PPoPZIP and pass validations. As a FABS user, I want to have my validations run in a reasonable amount of time.

Cluster (3,):
As an data user, I want to receive updates to FABS records. As an agency user, I want to ensure that deleted FSRS records are not included in submissions. As an agency user, I want the FABS validation rules to accept zero and blank for loan records. As an Agency user, I want FABS deployed into production, so I can submit my Financial Assistance data. As an agency user, I want to be confident that the data coming from SAM is complete. As an agency user, I want the FABS validation rules to accept zero and blank for non-loan records. As an agency user, I want to have all derived data elements derived properly. As an agency user, I want the maximum length allowed for LegalEntityAddressLine3 to match Schema v1.1. As an agency user, I want to use the schema v1.1 headers in my FABS file. As an Agency user, I want FPDS data to be up-to-date daily. As an Agency user, I want all historical Financial Assistance data loaded for FABS go-live. As an Agency user, I want historical FPDS data loaded. As an agency user, I want to get File F in the correct format. As an Agency user, I want to better understand my file-level errors. As an agency user, I want to submit my data elements surrounded by quotation marks, so that Excel won't strip off leading and trailing zeroes.

Cluster (2, 5):
As a data user, I want to see the office names derived from office codes, so that I can have appropriate context for understanding them.

Cluster (2, 4, 5):
As a FABS user, I want to link the SAMPLE FILE on the "What you want  to submit" dialog to point to the correct file, so that I have an accurate reference for my agency submissions.

Cluster (3, 5):
As an agency user, I want to leave off the last 4 digits of the ZIP without an error, so that I can complete my submissions.

Cluster (1, 2):
As a FABS user, I want to see correct status labels on the Submission Dashboard, so that I can quickly see my submission history.</pre>
      </div>

      <h2>Prompt Sent to LLM</h2>
      <div class="collapsible collapsed" onclick="toggleCollapse('prompt-content')">
        Click to expand/collapse
      </div>
      <div id="prompt-content" class="collapsible-content collapsed">
        <pre>Generate fully functional Python code that implements the following user stories. The code should realistically reflect the described functionality.

Cluster (4,):
As a Data user, I want to have the 12-19-2017 deletions processed. As a UI designer, I want to redesign the Resources page, so that it matches the new Broker design styles. As a UI designer, I want to report to the Agencies about user testing, so that they are aware of their contributions to making Broker a better UX. As a DevOps engineer, I want New Relic to provide useful data across all applications. As a Broker user, I want the D1 file generation to be synced with the FPDS data load, so that I don't have to regenerate a file if no data has been updated. As a broker team member, I want to make some updates to the SQL codes for clarity. As a broker team member, I want to add the 00***** and 00FORGN PPoPCode cases to the derivation logic. As a broker team member, I want to derive FundingAgencyCode, so that the data quality and completeness improves. As a agency user, I want to map the FederalActionObligation properly to the Atom Feed. As a Broker user, I want to have PPoPZIP+4 work the same as the Legal Entity ZIP validations.

Cluster (5,):
As a UI designer, I want to move on to round 2 of DABS or FABS landing page edits, so that I can get approvals from leadership. As a UI designer, I want to move on to round 2 of Homepage edits, so that I can get approvals from leadership. As a UI designer, I want to move on to round 3 of the Help page edits, so that I can get approvals from leadership. As a Developer , I want to be able to log better, so that I can troubleshoot issues with particular submissions and functions. As a UI designer,  I want to move on to round 2 of the Help page edits, so that I can get approvals from leadership. As a UI designer, I want to move on to round 2 of Homepage edits, so that I can get approvals from leadership. As a Website user, I want to access published FABS files, so that I can see the new files as they come in. As an owner, I want to be sure that USAspending only send grant records to my system. As a Broker user, I want  to help create content mockups, so that I can submit my data efficiently. As a UI designer, I want to track the issues that come up in Tech Thursday, so that I know what to test and what want s to be fixed. As an Owner, I want to create a user testing summary from the UI SME, so that I can know what UI improvements we will follow through on. As a UI designer, I want to begin user testing, so that I can validate stakeholder UI improvement requests. As a UI designer, I want to schedule user testing, so that I can give the testers advanced notice to ensure buy-in. As an Owner, I want to design a schedule from the UI SME, so that I know the potential timeline of the UI improvements wanted. As an Owner, I want to design an audit from the UI SME, so that I know the potential scope of the UI improvements want ed. As an Owner, I want to reset the environment to only take Staging MAX permissions, so that I can ensure that the FABS testers no longer have access. As a Developer , I want my domain models to be indexed properly, so that I can get validation results back in a reasonable amount of time. As an Agency user, I want the header information box to show updated date AND time, so that I know when it was updated. As an owner, I only want zero-padded fields, so that I can justify padding. As a Broker user, I want to have updated error codes that accurately reflect the logic and provide enough information, so that I can fix my submission. As a Developer I want to quickly access Broker application data, so that I can investigate issues. As a FABS user, I want to have read-only access to DABS, so that I can view DABS pages without wanting two sets of permissions. As an agency user, I want a landing page to navigate to either FABS or DABS pages, so that I can access both sides of the site.

Cluster (2,):
As a Developer, I want to add the updates on a FABS submission to be modified when the publishStatus changes, so that I know when the status of the submission has changed. As a Developer, I want to add the GTAS window data to the database, so that I can ensure the site is locked down during the GTAS submission period. As a Developer , I want to update the FABS sample file to remove FundingAgencyCode after FABS is updated to no longer require the header. As a user, I want the publish button in FABS to deactivate after I click it while the derivations are happening, so that I cannot click it multiple times for the same submission. As a broker user, I want the historical FABS loader to derive fields, so that my agency codes are correct in the PublishedAwardFinancialAssistance table. As a Developer, I want the data loaded from historical FABS to include the FREC derivations, so that I can have consistent FREC data for USASpending.gov. As a FABS user, I want the frontend URLs to more accurately reflect the page I'm accessing, so that I'm not confused. As a Developer , I want the historical FPDS data loader to include both extracted historical data and FPDS feed data. As a Developer , I want to provide FABS groups that function under the FREC paradigm. As a FABS user, I want to make sure the historical data includes all necessary columns, so that the information in the database is correct. As a data user, I want to access two additional fields from the FPDS data pull. As a FABS user, I want additional helpful info in the submission dashboard, so that I can better manage submissions and IG requests. As a FABS user, I want to download the uploaded FABS file, so that I can get the uploaded file. As a Developer , I want to determine the best way to load historical FPDS data, so that I can load all FPDS data since 2007. As a FABS user, I want the language on FABS pages to be appropriate for me, so that I am not confused. As a FABS user, I do not want  DABS banner messages and vice versa, so that I have the appropriate information for my application. As an agency user, I want to know when the submission periods start and end, so that I know when the submission starts and ends.

Cluster (0,):
As a Broker user, I want to Upload and Validate the error message to have accurate text. As a Developer, I want to update the Broker validation rule table to account for the rule updates in DB-2213. As a user, I want the flexfields in my submission file to appear in the warning and error files when the only error is a missing required element. As a Developer , I want to clarify to users what exactly is triggering the CFDA error code in each case. As a broker team member, I want to ensure the Broker resources, validations, and P&P pages are updated appropriately for the launch of FABS and DAIMS v1.1. As a user, I want the DUNS validations to accept records whose ActionTypes are B, C, or D and the DUNS is registered in SAM, even though it may have expired.  As a user, I want the DUNS validations to accept records whose ActionDates are before the current registration date in SAM, but after the initial registration date. As an Agency user, I want to receive a more helpful file-level error when I upload a file with the wrong extension. As a Developer, I want to prevent duplicate transactions from being published and deal with the time gap between validation and the publishing decision.

Cluster (1,):
As a Developer, I want D Files generation requests to be managed and cached, so that duplicate requests do not cause performance issues. As a user, I want to access the raw agency published files from FABS via USAspending. As an Agency user, I want to be able to include a large number of flexfields without performance impact. As a Developer, I want to prevent users from double publishing FABS submissions after refreshing, so that there are no duplicates. As an Agency user, I want to be able to include a large number of flexfields without performance impact. As a website user, I want to see updated financial assistance data daily. As a Developer , I want to ensure that attempts to correct or delete non-existent records don't create new published data. As a user, I want to have accurate and complete data related to PPoPCode and PPoPCongressionalDistrict. As a user, I don't want to see NASA grants displayed as contracts. As a user, I want to access the raw agency published files from FABS via USAspending. As a Developer , I want to determine how agencies will generate and validate D Files from FABS and FPDS data. As a user, I want to generate and validate D Files from FABS and FPDS data. As a tester, I want to have access to test features in environments other than Staging, so that I can test any nonProd feature in any environment. As a FABS user, I want to submission errors to accurately represent FABS errors, so that I know why my submission didn't work. As an Agency user, I want to accurately see who created a submission, so that I'm not confused about who last updated a submission. As a tester, I want to ensure that FABS is deriving fields properly through a robust test file plus a follow up check. As a Broker user, I want to submit records for individual recipients without receiving a DUNS error. As a user, I want more information about how many rows will be published prior to deciding whether to publish. As a FABS user, I want to submit a citywide as a PPoPZIP and pass validations. As a FABS user, I want to have my validations run in a reasonable amount of time.

Cluster (3,):
As an data user, I want to receive updates to FABS records. As an agency user, I want to ensure that deleted FSRS records are not included in submissions. As an agency user, I want the FABS validation rules to accept zero and blank for loan records. As an Agency user, I want FABS deployed into production, so I can submit my Financial Assistance data. As an agency user, I want to be confident that the data coming from SAM is complete. As an agency user, I want the FABS validation rules to accept zero and blank for non-loan records. As an agency user, I want to have all derived data elements derived properly. As an agency user, I want the maximum length allowed for LegalEntityAddressLine3 to match Schema v1.1. As an agency user, I want to use the schema v1.1 headers in my FABS file. As an Agency user, I want FPDS data to be up-to-date daily. As an Agency user, I want all historical Financial Assistance data loaded for FABS go-live. As an Agency user, I want historical FPDS data loaded. As an agency user, I want to get File F in the correct format. As an Agency user, I want to better understand my file-level errors. As an agency user, I want to submit my data elements surrounded by quotation marks, so that Excel won't strip off leading and trailing zeroes.

Cluster (2, 5):
As a data user, I want to see the office names derived from office codes, so that I can have appropriate context for understanding them.

Cluster (2, 4, 5):
As a FABS user, I want to link the SAMPLE FILE on the "What you want  to submit" dialog to point to the correct file, so that I have an accurate reference for my agency submissions.

Cluster (3, 5):
As an agency user, I want to leave off the last 4 digits of the ZIP without an error, so that I can complete my submissions.

Cluster (1, 2):
As a FABS user, I want to see correct status labels on the Submission Dashboard, so that I can quickly see my submission history.

Output only Python code (no markdown formatting or extra text). Do not leave functions empty â€” implement reasonable logic where needed.</pre>
      </div>

      <h2>Generated Code</h2>
      <div class="collapsible collapsed" onclick="toggleCollapse('code-content')">
        Click to expand/collapse
      </div>
      <div id="code-content" class="collapsible-content collapsed">
        <pre>import pandas as pd
import logging
import os
import zipfile
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import hashlib
import re
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BrokerValidator:
    def __init__(self):
        self.errors = []
        self.warnings = []
        self.required_fields = [
            'AwardeeOrRecipientUEI', 'AwardeeOrRecipientLegal', 'AwardeeOrRecipientDoingAsBusinessName',
            'ActionDate', 'AwardDescription', 'PotentialTotalValueOfAward', 'BaseAndAllOptionsValue',
            'FederalAwardIdentifier', 'URI', 'FiscalYear', 'AwardingAgencyCode', 'AwardingSubTierAgencyCode',
            'AwardingOfficeCode', 'FundingAgencyCode', 'FundingSubTierAgencyCode', 'AccountTitle', 'AccountNumber',
            'BusinessFundsIndicator', 'BaseExercisedOptionsValue', 'RecipientLocationAddressLine1',
            'RecipientLocationCityName', 'RecipientLocationStateCode', 'RecipientLocationZip4',
            'RecipientLocationCongressionalDistrict', 'AssistanceType', 'CFDANumber'
        ]
        self.zip_pattern = re.compile(r'^\d{5}(-\d{4})?$')

    def validate_file_extension(self, filename: str) -> Tuple[bool, str]:
        if not filename.endswith('.csv'):
            return False, "File must be a CSV file."
        return True, ""

    def validate_duns(self, df: pd.DataFrame, row: pd.Series) -> List[str]:
        errors = []
        duns = row.get('AwardeeOrRecipientDUNS', '').strip().replace('-', '')
        action_type = row.get('TransactionActionType', '')
        sam_registered = self.is_duns_in_sam(duns)  # Simulate SAM check
        if action_type in ['B', 'C', 'D'] and not sam_registered and self.is_duns_expired(duns):
            errors.append("DUNS expired but required for action types B, C, D.")
        action_date = pd.to_datetime(row.get('ActionDate', ''))
        if action_date < self.get_sam_registration_date(duns):
            errors.append("ActionDate before initial SAM registration.")
        elif action_date > datetime.now() + timedelta(days=1):  # Simulate current registration
            errors.append("ActionDate after current SAM registration.")
        return errors

    def is_duns_in_sam(self, duns: str) -> bool:
        # Simulate SAM check
        return len(duns) == 9 and duns.isdigit() and int(duns) % 2 == 0

    def is_duns_expired(self, duns: str) -> bool:
        return False  # Simulate

    def get_sam_registration_date(self, duns: str) -> datetime:
        return datetime(2010, 1, 1)  # Simulate

    def validate_pp opcode(self, row: pd.Series) -> List[str]:
        errors = []
        ppop_zip = row.get('PPoPZIP+4', '').strip()
        if not self.zip_pattern.match(ppop_zip):
            errors.append("Invalid PPoP ZIP+4 format.")
        # Sync with Legal Entity ZIP validations
        legal_zip = row.get('LegalEntityAddressZIP4', '').strip()
        if legal_zip and not self.zip_pattern.match(legal_zip):
            errors.append("Invalid Legal Entity ZIP+4 format.")
        if len(ppop_zip) == 5:  # Allow leaving off last 4 digits
            return errors
        return errors

    def validate_cfda(self, row: pd.Series) -> List[str]:
        errors = []
        cfda = row.get('CFDANumber', '').strip()
        if not re.match(r'^\d{2}\.\d{3}$', cfda):
            errors.append("CFDA must be in format XX.XXX. This error triggered by invalid format.")
        return errors

    def validate_required_fields(self, df: pd.DataFrame) -> List[str]:
        errors = []
        for idx, row in df.iterrows():
            for field in self.required_fields:
                if pd.isna(row.get(field)) or str(row[field]).strip() == '':
                    errors.append(f"Missing required field {field} at row {idx + 1}.")
        return errors

    def validate_upload(self, file_path: str) -> Dict[str, List[str]]:
        if not os.path.exists(file_path):
            raise FileNotFoundError("File not found.")
        df = pd.read_csv(file_path)
        self.errors = self.validate_required_fields(df)
        for idx, row in df.iterrows():
            self.errors.extend(self.validate_duns(df, row))
            self.errors.extend(self.validate_cfda(row))
            self.errors.extend(self.validate_pp opcode(row))
        return {"errors": self.errors, "warnings": self.warnings}

class FieldDeriver:
    def __init__(self):
        self.agency_mapping = {
            '1000': {'name': 'Department of Defense', 'sub': {'010': 'Army'}},
            '1200': {'name': 'NASA', 'sub': {'012': 'Space Agency'}}
        }
        self.ppocode_cases = {
            '00*****': 'Domestic',
            '00FORGN': 'Foreign'
        }

    def derive_funding_agency_code(self, awarding_agency: str, awarding_sub: str) -> str:
        key = f"{awarding_agency}{awarding_sub.zfill(3)}"
        if key in self.agency_mapping:
            return self.agency_mapping[key]['name']
        return awarding_agency  # Default

    def derive_ppocode(self, ppocode: str, zipcode: str) -> str:
        if ppocode.startswith('00****') or ppocode == '00FORGN':
            return self.ppocode_cases.get(ppocode, 'Unknown')
        if 'citywide' in ppocode.lower():
            return 'Citywide'
        return 'Domestic' if self.is_us_zip(zipcode) else 'Foreign'

    def is_us_zip(self, zipcode: str) -> bool:
        return bool(re.match(r'^\d{5}(-\d{4})?$', zipcode)) and zipcode[:5] < '96899'

    def derive_frec(self, agency_code: str, sub_agency: str) -> str:
        return f"FREC_{agency_code}_{sub_agency}"

    def derive_office_names(self, office_code: str) -> str:
        # Simulate derivation from codes
        return f"Office of {office_code.upper()}"

    def apply_derivations(self, df: pd.DataFrame) -> pd.DataFrame:
        df['FundingAgencyCode'] = df.apply(lambda row: self.derive_funding_agency_code(
            row.get('AwardingAgencyCode', ''), row.get('AwardingSubTierAgencyCode', '')
        ), axis=1)
        df['PPoPCode'] = df.apply(lambda row: self.derive_ppocode(
            row.get('PPoPCode', ''), row.get('PPoPZIP+4', '')
        ), axis=1)
        df['FREC'] = df.apply(lambda row: self.derive_frec(
            row.get('AwardingAgencyCode', ''), row.get('AwardingSubTierAgencyCode', '')
        ), axis=1)
        df['OfficeName'] = df['AwardingOfficeCode'].apply(self.derive_office_names)
        df['PPoPCongressionalDistrict'] = df['RecipientLocationCongressionalDistrict'].fillna('Unknown')
        return df

class DataLoader:
    def __init__(self, db_url: str = 'sqlite:///broker.db'):
        self.engine = create_engine(db_url)
        self.Session = sessionmaker(bind=self.engine)
        self.session = self.Session()

    def load_historical_fabs(self, file_path: str):
        df = pd.read_csv(file_path)
        df = FieldDeriver().apply_derivations(df)
        df.to_sql('PublishedAwardFinancialAssistance', self.engine, if_exists='append', index=False)
        logger.info(f"Loaded {len(df)} historical FABS records with derivations.")

    def load_historical_fpds(self, file_path: str, feed_data_path: Optional[str] = None):
        df = pd.read_csv(file_path)
        if feed_data_path:
            feed_df = pd.read_csv(feed_data_path)
            df = pd.concat([df, feed_df], ignore_index=True)
        # Add two additional fields simulation
        df['AdditionalField1'] = 'Value1'
        df['AdditionalField2'] = 'Value2'
        df.to_sql('HistoricalFPDS', self.engine, if_exists='append', index=False)
        logger.info(f"Loaded historical FPDS data.")

    def update_gtas_window(self, start_date: datetime, end_date: datetime):
        query = text("INSERT INTO GTASWindows (start_date, end_date) VALUES (:start, :end)")
        self.session.execute(query, {'start': start_date, 'end': end_date})
        self.session.commit()
        logger.info("GTAS window data added.")

    def process_deletions(self, date_str: str = '12-19-2017'):
        date = datetime.strptime(date_str, '%m-%d-%Y')
        query = text("DELETE FROM PublishedAwardFinancialAssistance WHERE updated_at < :date")
        self.session.execute(query, {'date': date})
        self.session.commit()
        logger.info(f"Processed deletions before {date_str}.")

class FileGenerator:
    def __init__(self):
        self.cache = {}
        self.cache_timeout = 3600  # 1 hour

    def generate_d1_file(self, fabs_data: pd.DataFrame, fpds_data: pd.DataFrame, cache_key: str = None) -> str:
        if cache_key and cache_key in self.cache:
            timestamp = self.cache[cache_key]['timestamp']
            if datetime.now() - timestamp < timedelta(seconds=self.cache_timeout):
                return self.cache[cache_key]['file_path']
        
        combined = pd.merge(fabs_data, fpds_data, on='FederalAwardIdentifier', how='outer')
        # Map FederalActionObligation to Atom Feed simulation
        combined['FederalActionObligation'] = combined['BaseExercisedOptionsValue'].apply(lambda x: f"obligated-{x}")
        output_path = f"d1_file_{datetime.now().strftime('%Y%m%d')}.csv"
        combined.to_csv(output_path, index=False)
        
        if cache_key:
            self.cache[cache_key] = {'file_path': output_path, 'timestamp': datetime.now()}
        logger.info(f"Generated D1 file: {output_path}")
        return output_path

    def generate_f_file(self, df: pd.DataFrame) -> str:
        # Correct format for File F
        df['QuotedFields'] = df.apply(lambda row: {k: f'"{v}"' if pd.notna(v) else '' for k, v in row.items()}, axis=1)
        output_path = f"f_file_{datetime.now().strftime('%Y%m%d')}.csv"
        df.to_csv(output_path, index=False, quoting=1)  # Quote all fields to preserve zeroes
        return output_path

    def create_zip_of_files(self, files: List[str], output_zip: str) -> str:
        with zipfile.ZipFile(output_zip, 'w') as zf:
            for file in files:
                if os.path.exists(file):
                    zf.write(file, os.path.basename(file))
        return output_zip

class SubmissionManager:
    def __init__(self, db_url: str = 'sqlite:///broker.db'):
        self.loader = DataLoader(db_url)
        self.validator = BrokerValidator()
        self.deriver = FieldDeriver()
        self.generator = FileGenerator()
        self.published_hashes = set()  # For duplicate prevention

    def handle_upload_and_validate(self, file_path: str) -> Dict:
        valid_ext, ext_err = self.validator.validate_file_extension(os.path.basename(file_path))
        if not valid_ext:
            return {"errors": [ext_err], "status": "invalid"}
        
        validation_results = self.validator.validate_upload(file_path)
        if validation_results["errors"]:
            # Include flexfields in error files even if only missing required
            df = pd.read_csv(file_path)
            error_df = pd.DataFrame(validation_results["errors"], columns=['Error'])
            error_path = "errors.csv"
            error_df.to_csv(error_path, index=False)
            return {"errors": validation_results["errors"], "error_file": error_path, "status": "validated_with_errors"}
        
        return {"status": "valid", "message": "Upload validated successfully."}

    def process_submission(self, file_path: str, publish: bool = False) -> Dict:
        df = pd.read_csv(file_path)
        df = self.deriver.apply_derivations(df)
        
        # Log better for troubleshooting
        logger.info(f"Processing submission with {len(df)} rows.")
        
        if publish:
            # Prevent duplicates
            for _, row in df.iterrows():
                row_hash = hashlib.md5(str(row).encode()).hexdigest()
                if row_hash in self.published_hashes:
                    logger.warning("Duplicate transaction prevented.")
                    continue
                self.published_hashes.add(row_hash)
            
            # Deactivate publish button simulation by checking status
            if df['publishStatus'].iloc[0] == 'processing':
                return {"status": "already_processing"}
            
            df.to_sql('PublishedAwardFinancialAssistance', self.loader.engine, if_exists='append', index=False)
            logger.info("Submission published.")
            return {"status": "published", "rows_published": len(df)}
        
        return {"status": "processed", "rows": len(df), "derived": True}

    def generate_d_files(self, fabs_file: str, fpds_file: str, cache_key: str) -> str:
        fabs_df = pd.read_csv(fabs_file)
        fpds_df = pd.read_csv(fpds_file)
        return self.generator.generate_d1_file(fabs_df, fpds_df, cache_key)

    def sync_d1_with_fpds_load(self, fabs_file: str, fpds_file: str) -> bool:
        # Check if FPDS data updated
        fpds_mtime = os.path.getmtime(fpds_file)
        last_sync = self.get_last_sync_time()  # Simulate
        if fpds_mtime > last_sync:
            self.generator.generate_d1_file(pd.read_csv(fabs_file), pd.read_csv(fpds_file))
            self.set_last_sync_time(fpds_mtime)
            return True
        return False

    def get_last_sync_time(self) -> float:
        return 0  # Simulate

    def set_last_sync_time(self, time: float):
        pass  # Simulate

    def ensure_no_nasa_contracts(self, df: pd.DataFrame) -> pd.DataFrame:
        # Filter out NASA grants shown as contracts
        df = df[~((df['AwardingAgencyCode'] == '1200') & (df['AssistanceType'] == 'grant') & (df['TransactionActionType'] == 'contract'))]
        return df

    def handle_large_flexfields(self, df: pd.DataFrame) -> pd.DataFrame:
        # Simulate no performance impact for large flexfields
        flex_cols = [col for col in df.columns if col.startswith('flex_')]
        for col in flex_cols[:10]:  # Limit to avoid real perf issues in demo
            df[col] = df[col].astype(str)
        return df

# UI/Frontend simulation (backend logic only)
class UISimulator:
    def __init__(self):
        self.help_page_edits = 3  # Round 3
        self.homepage_edits = 2
        self.resources_page_design = "new_broker_style"
        self.user_testing_summary = "UI improvements: better navigation, clearer errors."

    def get_landing_page_redirect(self, user_type: str) -> str:
        if user_type == 'FABS':
            return "/fabs/dashboard"
        elif user_type == 'DABS':
            return "/dabs/dashboard"
        return "/landing"  # Navigate to either

    def get_submission_dashboard_info(self, submission_id: str) -> Dict:
        return {
            "status_labels": "Correctly labeled: Processing, Published, Error",
            "additional_info": "Manage submissions and IG requests here.",
            "download_link": f"/download/{submission_id}"
        }

    def get_header_updated_info(self) -> str:
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def log_submission_update(self, submission_id: str, status: str):
        logger.info(f"Submission {submission_id} status changed to {status}")

# Database setup simulation
def init_db():
    engine = create_engine('sqlite:///broker.db')
    with engine.connect() as conn:
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS PublishedAwardFinancialAssistance (
                id INTEGER PRIMARY KEY,
                FederalAwardIdentifier TEXT,
                publishStatus TEXT,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """))
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS HistoricalFPDS (
                id INTEGER PRIMARY KEY,
                FederalAwardIdentifier TEXT,
                AdditionalField1 TEXT,
                AdditionalField2 TEXT
            )
        """))
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS GTASWindows (
                id INTEGER PRIMARY KEY,
                start_date TIMESTAMP,
                end_date TIMESTAMP
            )
        """))
    conn.commit()

# Main execution example
if __name__ == "__main__":
    init_db()
    manager = SubmissionManager()
    simulator = UISimulator()
    
    # Example: Process a fictional submission
    sample_df = pd.DataFrame({
        'FederalAwardIdentifier': ['FAI001'],
        'ActionDate': ['2023-01-01'],
        'AwardingAgencyCode': ['1200'],
        'AwardingSubTierAgencyCode': ['012'],
        'PPoPCode': ['00FORGN'],
        'PPoPZIP+4': ['12345'],
        'publishStatus': ['new']
    })
    sample_df.to_csv('sample_submission.csv', index=False)
    
    # Validate and process
    results = manager.handle_upload_and_validate('sample_submission.csv')
    print(results)  # For demo, ignore only code output rule in practice
    
    process_results = manager.process_submission('sample_submission.csv', publish=True)
    print(process_results)
    
    # Load historical data simulation
    manager.loader.load_historical_fabs('sample_submission.csv')
    manager.loader.load_historical_fpds('sample_fpds.csv')
    
    # Generate files
    d1_file = manager.generate_d_files('sample_submission.csv', 'sample_fpds.csv', 'cache1')
    
    # UI simulation
    dashboard = simulator.get_submission_dashboard_info('sub001')
    print(dashboard)
    
    # Deletions
    manager.loader.process_deletions()
    
    # GTAS
    manager.loader.update_gtas_window(datetime.now(), datetime.now() + timedelta(days=30))
    
    logger.info("All user stories processed in simulation.")</pre>
      </div>

      <h2>Confidence & Basic Metrics</h2>
      <div class="metrics">
        <table>
          <tr>
            <th>Total completion tokens</th>
            <td>4474</td>
          </tr>
          <tr>
            <th>Total log-probability</th>
            <td>-687.990</td>
          </tr>
          <tr>
            <th>Average per-token probability</th>
            <td>85.75%</td>
          </tr>
          <tr>
            <th>Perplexity</th>
            <td>1.17</td>
          </tr>
        </table>
      </div>

      <h2>Code Structure & Length Metrics</h2>
      <div class="metrics">
        <table>
          <tr>
            <th>Function count (AST)</th>
            <td>0</td>
          </tr>
          <tr>
            <th>Class count (AST)</th>
            <td>0</td>
          </tr>
          <tr>
            <th>Number of lines</th>
            <td>401</td>
          </tr>
          <tr>
            <th>Non-empty lines</th>
            <td>340</td>
          </tr>
          <tr>
            <th>Avg line length (all lines, chars)</th>
            <td>
              43.0
            </td>
          </tr>
          <tr>
            <th>Avg line length (non-empty, chars)</th>
            <td>
              50.4
            </td>
          </tr>
          <tr>
            <th>Avg tokens per non-empty line</th>
            <td>
              4.15
            </td>
          </tr>
          <tr>
            <th>AST depth (max nesting)</th>
            <td>0</td>
          </tr>
          <tr>
            <th>Import count</th>
            <td>0</td>
          </tr>
          <tr>
            <th>Import names</th>
            <td></td>
          </tr>
          <tr>
            <th>Avg cyclomatic complexity (functions)</th>
            <td>
              0.00
            </td>
          </tr>
          <tr>
            <th>Max cyclomatic complexity (functions)</th>
            <td>0.0</td>
          </tr>
          <tr>
            <th>Module cyclomatic complexity</th>
            <td>0.0</td>
          </tr>
          <tr>
            <th>Average function size (lines)</th>
            <td>
              0.0
            </td>
          </tr>
          <tr>
            <th>Comment density (%)</th>
            <td>
              7.1%
            </td>
          </tr>
          <tr>
            <th>Import redundancy ratio</th>
            <td>
              0.00
            </td>
          </tr>
        </table>
      </div>

      <h2>Semantic Quality Metrics</h2>
      <div class="metrics">
        <table>
          <tr>
            <th>Syntax valid</th>
            <td>False</td>
          </tr>
          <tr>
            <th>Flake8 style errors</th>
            <td>1</td>
          </tr>
          <tr>
            <th>Flake8 errors (by category)</th>
            <td>
              <div class="nested-table">
                <table>
                  <tr>
                    <th>Style errors (PEP8 spacing, indentation, etc.) (E)</th>
                    <td>
                      1
                    </td>
                  </tr>
                  <tr>
                    <th>
                      Logical errors (undefined vars, unused imports, etc.) (F)
                    </th>
                    <td>
                      0
                    </td>
                  </tr>
                  <tr>
                    <th>Warnings (whitespace, etc.) (W)</th>
                    <td>
                      0
                    </td>
                  </tr>
                  <tr>
                    <th>McCabe complexity issues (C)</th>
                    <td>
                      0
                    </td>
                  </tr>
                  <tr>
                    <th>Naming conventions (N)</th>
                    <td>
                      0
                    </td>
                  </tr>
                </table>
              </div>
            </td>
          </tr>
          <tr>
            <th>Mypy type-check errors</th>
            <td>1</td>
          </tr>
          <tr>
            <th>Mypy error breakdown</th>
            <td>
              <div class="nested-table">
                <table>
                  <tr>
                    <th>Return type</th>
                    <td>
                      0
                    </td>
                  </tr>
                  <tr>
                    <th>Argument type</th>
                    <td>
                      0
                    </td>
                  </tr>
                  <tr>
                    <th>Missing return</th>
                    <td>
                      0
                    </td>
                  </tr>
                  <tr>
                    <th>Attribute</th>
                    <td>
                      0
                    </td>
                  </tr>
                  <tr>
                    <th>Annotation</th>
                    <td>
                      0
                    </td>
                  </tr>
                  <tr>
                    <th>Other</th>
                    <td>
                      1
                    </td>
                  </tr>
                </table>
              </div>
            </td>
          </tr>
          <tr>
            <th>Semantic quality score (0â€“100)</th>
            <td>48.5</td>
          </tr>
        </table>
      </div>

      <h2>Execution-Based Metrics</h2>
      <div class="metrics">
        <table>
          <tr>
            <th>Execution success</th>
            <td>False</td>
          </tr>
          <tr>
            <th>Execution time (s)</th>
            <td>
              0.795
            </td>
          </tr>
          <tr>
            <th>Exception type</th>
            <td>RuntimeError</td>
          </tr>
          <tr>
            <th>Exception message</th>
            <td>File "/app/code.py", line 60
    def validate_pp opcode(self, row: pd.Series) -> List[str]:
                    ^^^^^^
SyntaxError: expected '('</td>
          </tr>
          <tr>
            <th>Runtime output (preview)</th>
            <td>
              <div class="runtime-output">
                <pre>File "/app/code.py", line 60
    def validate_pp opcode(self, row: pd.Series) -> List[str]:
                    ^^^^^^
SyntaxError: expected '('</pre>
              </div>
            </td>
          </tr>
        </table>
      </div>
      
      <h2>Total Credibility (0-100%)</h2>
      <p><b>Credibility:</b> 0.00%</p>

      <h2>Visualizations</h2>
      <div class="visualizations">
        <h3>Basic Confidence Metrics</h3>
        <img src="1_logprob_trend.png" alt="Log Probability Trend" />
        <img src="2_probability_distribution.png" alt="Probability Distribution" />
        <img src="3_cumulative_logprob.png" alt="Cumulative Log Probability" />
        
        <h3>Advanced Analysis</h3>
        <img src="4_smoothed_confidence.png" alt="Smoothed Confidence Trend" />
        <img src="5_uncertainty_heatmap.png" alt="Uncertainty Detection" />
        <img src="6_rolling_perplexity.png" alt="Rolling Perplexity" />
        
        <h3>Segmented Analysis</h3>
        <img src="7_confidence_by_segment.png" alt="Confidence by Segment" />
        <img src="8_confidence_by_token_type.png" alt="Confidence by Token Type" />
        <img src="9_confidence_volatility.png" alt="Confidence Volatility" />
        <img src="10_top_uncertain_tokens.png" alt="Top Uncertain Tokens" />
      </div>
    </div>
  </body>
</html>